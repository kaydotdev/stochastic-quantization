\section{Introduction}

Quantization and clustering are fundamental encoding techniques that provide compact representations of original data \cite{Graf_Luschgy_2000,Jain_2010,Scholkopf_Smola_2002}. Clustering algorithms have emerged as prominent tools for unsupervised learning, with recent applications spanning diverse domains such as location-allocation problems \cite{Wang_Wei_2020}, document classification \cite{Radomirovic_2023,Widodo_2011}, and data compression \cite{Wan_2019}.

In this context, we consider a random variable $\xi$ with values in Euclidean space $\mathbb{R}^n$ and distribution $P(d\xi)$, representing the original distribution. The encoded discrete distribution is parameterized by a set of atoms $\{y_1, \ldots, y_K\}$ with corresponding probabilities $\{q_1, \ldots, q_K\}$. The optimal quantization problem aims to find the encoded distribution that minimizes the distance to the original distribution. This mathematical structure is analogous to the optimal clustering problem, where the objective is to determine the positions of $K$ cluster centers $\{y_1, \ldots, y_K\}$ such that the sum of distances from each element $\xi$ to the nearest cluster center is minimized.

The K-means algorithm, proposed by Lloyd \cite{Lloyd_1982}, has been widely used for solving quantization and clustering problems, with numerous extensions \cite{Jain_2010}. Bottou and Bengio \cite{Bottou_1994} interpreted the K-means algorithm as an analogue of Newton's method and proposed several stochastic gradient descent algorithms for solving optimal quantization and clustering problems. However, traditional clustering algorithms are limited by the requirement to load all training data into memory, rendering them non-scalable for large datasets. To address this limitation, Sculley \cite{Sculley_2010} introduced the Mini-batch K-means algorithm, which utilizes only a small subset of possible $\xi$ values at each iteration.

The Stochastic Quantization algorithm reframes the clustering problem as a stochastic transportation problem \cite{Kuzmenko_Uryasev_2019,Lakshmanan_Pichler_2023} by minimizing the distance between elements of the original distribution $\{\xi\}$ and atoms of the encoded discrete distribution $\{y_k\}$. This approach employs Stochastic Gradient Descent (SGD) \cite{ermoliev1976stochastic,kiefer1952stochastic,Robbins_Monro_1951} to search for an optimal minimum, leveraging its computational efficiency in large-scale machine learning problems \cite{Bottou_2010}. The use of stochastic approximation allows the algorithm to update parameters with only one element $\xi$ per iteration, ensuring memory efficiency without compromising convergence to the minimum \cite{Newton_Yousefian_Pasupathy_2018}.

This paper explores advanced modifications of the Stochastic Quantization algorithm, incorporating accelerated variants of SGD \cite{nesterov1983method,Poliak_1987,walkington_2023} and adaptive learning rate techniques \cite{Duchi_Hazan_Singer_2011,kingma2017adam,tieleman2012rmsprop} to enhance convergence speed. Norkin et al. \cite{Norkin_Kozyriev_Norkin_2024} provide a comprehensive comparison of various SGD variants, highlighting their respective advantages and limitations, while also offering convergence speed estimations.

Given that the optimal quantization problem is non-smooth and non-convex, specialized methods are required \cite{Gandikota_Kane_Maity_Mazumdar_2022,Tang_2017,Zhao_Lan_Chen_Ngo_2021}. To validate its convergence, we apply the general theory of non-smooth, non-convex stochastic optimization \cite{Ermoliev_Norkin_2003,Ermolev_Norkin_1998,mikhalevich2024}. While traditional clustering algorithms lack theoretical foundations for convergence guarantees and rely primarily on seeding techniques \cite{Arthur_Vassilvitskii_2007}, stochastic optimization theory provides specific conditions for the local convergence of the Stochastic Quantization algorithm, which we supplement in this research.

This paper introduces a novel approach to address semi-supervised learning challenges on high-dimensional data by integrating the Stochastic Quantization algorithm with a deep learning model based on the Triplet Network architecture \cite{Hoffer_2015}. The proposed method encodes images into low-dimensional representations in the $\mathbb{R}^3$ latent space, generating meaningful encoded features for the Stochastic Quantization algorithm. By employing the Triplet Network, this approach overcomes the limitations of quantization and clustering algorithms in high-dimensional spaces, such as visualization difficulties and decreased precision as the number of dimensions increases \cite{Kriegel_Kr√∂ger_Zimek_2009}.

To illustrate the efficiency and scalability of the Stochastic Quantization algorithm, we conducted experiments on a semi-supervised image classification problem using partially labeled data from the MNIST dataset \cite{lecun2010mnist}. The Triplet Network is initially trained on the labeled portion of the dataset as a supervised learning model. Subsequently, the trained network is utilized to project the remaining unlabeled data onto the latent space, which serves as input for training the Stochastic Quantization algorithm. The performance of the proposed solution is evaluated using the F1-score metric \cite{Chinchor_1992} for multi-label classification across various ratios of labeled to unlabeled data.
