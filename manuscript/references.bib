@inproceedings{Arthur_Vassilvitskii_2007,
	title        = {k-means++: the advantages of careful seeding},
	author       = {Arthur, David and Vassilvitskii, Sergei},
	year         = 2007,
	booktitle    = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
	location     = {New Orleans, Louisiana},
	publisher    = {Society for Industrial and Applied Mathematics},
	address      = {USA},
	series       = {SODA '07},
	pages        = {1027–1035},
	isbn         = 9780898716245,
	abstract     = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	numpages     = 9,
}

@article{Babic_2020,
	title        = {A Comparison of Approaches for Measuring the Semantic Similarity of Short Texts Based on Word Embeddings},
	author       = {Babić, Karlo and Guerra, Francesco and Martinčić-Ipšić, Sanda and Meštrović, Ana},
	year         = 2020,
	month        = {Dec.},
	journal      = {Journal of Information and Organizational Sciences},
	volume       = 44,
	number       = 2,
	doi          = {10.31341/jios.44.2.2},
	url          = {//jios.foi.hr/index.php/jios/article/view/1427},
	abstractnote = {&amp;lt;p&amp;gt;Measuring the semantic similarity of texts has a vital role in various tasks from the field of natural language processing. In this paper, we describe a set of experiments we carried out to evaluate and compare the performance of different approaches for measuring the semantic similarity of short texts. We perform a comparison of four models based on word embeddings: two variants of Word2Vec (one based on Word2Vec trained on a specific dataset and the second extending it with embeddings of word senses), FastText, and TF-IDF. Since these models provide word vectors, we experiment with various methods that calculate the semantic similarity of short texts based on word vectors. More precisely, for each of these models, we test five methods for aggregating word embeddings into text embedding. We introduced three methods by making variations of two commonly used similarity measures. One method is an extension of the cosine similarity based on centroids, and the other two methods are variations of the Okapi BM25 function. We evaluate all approaches on the two publicly available datasets: SICK and Lee in terms of the Pearson and Spearman correlation. The results indicate that extended methods perform better from the original in most of the cases.&amp;lt;/p&amp;gt;},
}

@inproceedings{Bottou_1994,
	title        = {Convergence Properties of the K-Means Algorithms},
	author       = {L'eon Bottou and Yoshua Bengio},
	year         = 1994,
	booktitle    = {Advances in Neural Information Processing Systems 7, {[NIPS} Conference, Denver, Colorado, USA, 1994]},
	publisher    = {{MIT} Press},
	pages        = {585--592},
	editor       = {Gerald Tesauro and David S. Touretzky and Todd K. Leen},
}

@article{Bottou_2010,
	title        = {Large-scale machine learning with stochastic gradient descent},
	author       = {Bottou, Léon},
	year         = 2010,
	journal      = {Proceedings of COMPSTAT’2010},
	pages        = {177–186},
	doi          = {10.1007/978-3-7908-2604-3},
}

@article{Bottou_Curtis_Nocedal_2018,
	title        = {Optimization methods for large-scale machine learning},
	author       = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	year         = 2018,
	journal      = {SIAM Review},
	volume       = 60,
	number       = 2,
	pages        = {223–311},
	doi          = {10.1137/16m1080173},
}

@inproceedings{Chinchor_1992,
	title        = {MUC-4 evaluation metrics},
	author       = {Chinchor, Nancy},
	year         = 1992,
	booktitle    = {Proceedings of the 4th Conference on Message Understanding},
	location     = {McLean, Virginia},
	publisher    = {Association for Computational Linguistics},
	address      = {USA},
	series       = {MUC4 '92},
	pages        = {22–29},
	doi          = {10.3115/1072064.1072067},
	isbn         = 1558602739,
	abstract     = {The MUC-4 evaluation metrics measure the performance of the message understanding systems. This paper describes the scoring algorithms used to arrive at the metrics as well as the improvements that were made to the MUC-3 methods. MUC-4 evaluation metrics were stricter than those used in MUC-3. Given the differences in scoring between MUC-3 and MUC-4, the MUC-4 systems' scores represent a larger improvement over MUC-3 performance than the numbers themselves suggest.},
	numpages     = 8,
}

@article{Duchi_2011,
	title        = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	author       = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year         = 2011,
	month        = {jul},
	journal      = {J. Mach. Learn. Res.},
	publisher    = {JMLR.org},
	volume       = 12,
	number       = {null},
	pages        = {2121–2159},
	doi          = {10.5555/1953048.2021068},
	issn         = {1532-4435},
	issue_date   = {2/1/2011},
	abstract     = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	numpages     = 39,
}

@article{Duchi_Hazan_Singer_2011,
	title        = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	author       = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year         = 2011,
	month        = {07},
	journal      = {Journal of Machine Learning Research},
	volume       = 12,
	pages        = {2121--2159},
}

@article{Ermolev_Norkin_1998,
	title        = {Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization},
	author       = {Ermol’ev, Yu. M. and Norkin, V. I.},
	year         = 1998,
	month        = {Mar},
	journal      = {Cybernetics and Systems Analysis},
	volume       = 34,
	number       = 2,
	pages        = {196–215},
	doi          = {10.1007/bf02742069},
}

@article{Ermoliev_Norkin_2003,
	title        = {Solution of nonconvex nonsmooth stochastic optimization problems},
	author       = {Ermoliev, Yu. M. and Norkin, V. I.},
	year         = 2003,
	month        = {Sep},
	journal      = {Cybernetics and Systems Analysis},
	volume       = 39,
	number       = 5,
	pages        = {701–715},
	doi          = {10.1023/b:casa.0000012091.84864.65},
}

@book{ermoliev1976stochastic,
	title        = {Stochastic Programming Methods},
	author       = {Ermoliev, Y.},
	year         = 1976,
	publisher    = {Nauka},
	address      = {Moscow},
}

@article{Gandikota_Kane_Maity_Mazumdar_2022,
	title        = {VqSGD: Vector quantized stochastic gradient descent},
	author       = {Gandikota, Venkata and Kane, Daniel and Maity, Raj Kumar and Mazumdar, Arya},
	year         = 2022,
	month        = {Jul},
	journal      = {IEEE Transactions on Information Theory},
	volume       = 68,
	number       = 7,
	pages        = {4573–4587},
	doi          = {10.1109/tit.2022.3161620},
}

@article{Graf_Luschgy_2000,
	title        = {Foundations of quantization for probability distributions},
	author       = {Graf, Siegfried and Luschgy, Harald},
	year         = 2000,
	journal      = {Lecture Notes in Mathematics},
	series       = 7,
	volume       = 1730,
	number       = 1,
	doi          = {10.1007/bfb0103945},
	collection   = 7,
}

@inproceedings{Hoffer_2015,
	title        = {Deep Metric Learning Using Triplet Network},
	author       = {Hoffer, Elad and Ailon, Nir},
	year         = 2015,
	booktitle    = {Similarity-Based Pattern Recognition},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {84--92},
	isbn         = {978-3-319-24261-3},
	editor       = {Feragen, Aasa and Pelillo, Marcello and Loog, Marco},
	abstract     = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
}

@article{Jain_2010,
	title        = {Data clustering: 50 years beyond K-means},
	author       = {Jain, Anil K.},
	year         = 2010,
	month        = {Jun},
	journal      = {Pattern Recognition Letters},
	volume       = 31,
	number       = 8,
	pages        = {651–666},
	doi          = {10.1016/j.patrec.2009.09.011},
}

@article{kiefer1952stochastic,
	title        = {Stochastic estimation of the maximum of a regression function},
	author       = {Kiefer, Jack and Wolfowitz, Jacob},
	year         = 1952,
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {JSTOR},
	pages        = {462--466},
}

@misc{kingma2017adam,
	title        = {Adam: A Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = 2017,
	eprint       = {1412.6980},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
}

@article{Kriegel_Kröger_Zimek_2009,
	title        = {Clustering high-dimensional data},
	author       = {Kriegel, Hans-Peter and Kröger, Peer and Zimek, Arthur},
	year         = 2009,
	month        = {Mar},
	journal      = {ACM Transactions on Knowledge Discovery from Data},
	volume       = 3,
	number       = 1,
	pages        = {1–58},
	doi          = {10.1145/1497577.1497578},
}

@article{Kuzmenko_Uryasev_2019,
	title        = {Kantorovich–Rubinstein Distance Minimization: Application to location problems},
	author       = {Kuzmenko, Viktor and Uryasev, Stan},
	year         = 2019,
	month        = sep,
	journal      = {Springer Optimization and Its Applications},
	volume       = 149,
	pages        = {59–68},
	doi          = {10.1007/978-3-030-22788-3\_3},
}

@article{Lakshmanan_Pichler_2023,
	title        = {Soft quantization using entropic regularization},
	author       = {Lakshmanan, Rajmadan and Pichler, Alois},
	year         = 2023,
	month        = {Oct},
	journal      = {Entropy},
	volume       = 25,
	number       = 10,
	pages        = 1435,
	doi          = {10.3390/e25101435},
}

@article{lecun2010mnist,
	title        = {MNIST handwritten digit database},
	author       = {LeCun, Yann and Cortes, Corinna and Burges, CJ},
	year         = 2010,
	journal      = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
	volume       = 2,
}

@article{Lloyd_1982,
	title        = {Least squares quantization in PCM},
	author       = {Lloyd, S.},
	year         = 1982,
	month        = {Mar},
	journal      = {IEEE Transactions on Information Theory},
	volume       = 28,
	number       = 2,
	pages        = {129–137},
	doi          = {10.1109/tit.1982.1056489},
}

@misc{mikhalevich2024,
	title        = {Methods of Nonconvex Optimization},
	author       = {V. S. Mikhalevich and A. M. Gupal and V. I. Norkin},
	year         = 2024,
	url          = {https://arxiv.org/abs/2406.10406},
	eprint       = {2406.10406},
	archiveprefix = {arXiv},
	primaryclass = {math.OC},
}

@inproceedings{nesterov1983method,
	title        = {A method of solving a convex programming problem with convergence rate {$O(1/k^2)$}},
	author       = {Nesterov, Yurii Evgen'evich},
	year         = 1983,
	booktitle    = {Doklady Akademii Nauk},
	volume       = 269,
	pages        = {543--547},
	organization = {Russian Academy of Sciences},
}

@article{Newton_Yousefian_Pasupathy_2018,
	title        = {Stochastic gradient descent: Recent trends},
	author       = {Newton, David and Yousefian, Farzad and Pasupathy, Raghu},
	year         = 2018,
	month        = {Oct},
	journal      = {Recent Advances in Optimization and Modeling of Contemporary Problems},
	pages        = {193–220},
	doi          = {10.1287/educ.2018.0191},
}

@article{Nguyen_Duong_2018,
	title        = {K-means** - a fast and efficient K-means algorithms},
	author       = {Nguyen, Cuong Duc and Duong, Trong Hai},
	year         = 2018,
	journal      = {International Journal of Intelligent Information and Database Systems},
	volume       = 11,
	number       = 1,
	pages        = 27,
	doi          = {10.1504/ijiids.2018.091595},
}

@article{Norkin_1986,
	title        = {Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization},
	author       = {Norkin, V. I.},
	year         = 1986,
	journal      = {Cybernetics},
	volume       = 22,
	number       = 6,
	pages        = {804–809},
	doi          = {10.1007/bf01068698},
}

@article{Norkin_Kozyriev_Norkin_2024,
	title        = {Modern stochastic quasi-gradient optimization algorithms},
	author       = {Norkin, Vladimir and Kozyriev, Anton and Norkin, Bogdan},
	year         = 2024,
	month        = {Mar.},
	journal      = {International Scientific Technical Journal "Problems of Control and Informatics"},
	volume       = 69,
	number       = 2,
	pages        = {71–83},
	doi          = {10.34229/1028-0979-2024-2-6},
	url          = {https://jais.net.ua/index.php/files/article/view/228},
	abstractnote = {&amp;lt;p&amp;gt;Stochastic optimization has become a leading method in various fields such as machine learning, neural networks training, and signal processing. These problems are aimed at minimizing the objective function with noisy and uncertain data. Such problems are attributed to stochastic programming. The article comprehensively compares modern quasi-gradient methods of stochastic optimization, illustrates their basic principles, convergence properties, and practical applications. First, basic concepts of gradient descent, stochastic approximation and optimization are introduced, and then optimization methods are explained in detail. Extensions of the basic gradient descent such as Nemirovski’s mirror decent, Polyak’s heavy ball (momentum) and Nesterov’s valley step methods are reviewed. Beside these classical methods, adaptive stochastic gradient methods are analyzed in depth; attention is focused on their ability to dynamically change the learning rate and decent directions depending on the structure of the problem and a course of optimization. The nomenclature of adaptive stochastic gradient methods includes AdaGrad, RMSProp, ADAM. Generalizations of these methods to the case of non-smooth objective function are studied; problems arising in non-smooth optimization landscapes are described. These generalizations exploit the idea of smoothing coming back to Steklov (1907) and consist in approximation of the original objective function by a sequence of close smoothed functions. The latter admit approximation of their gradients in the form of finite differences in random directions. The application of these improved methods in the context of unconditional optimization problems is illustrated and their effectiveness in accelerating convergence and increasing accuracy is demonstrated. In particular, our experiments demonstrate a considerable positive effect of smoothing on the behavior of the methods in case of nonsmooth problems. This benchmarking study aims to provide researchers and practitioners with a deeper understanding of recent advances in stochastic optimization and outline a path for future innovation.&amp;lt;/p&amp;gt;},
}

@article{Norkin_Onishchenko_2005,
	title        = {Minorant methods of stochastic global optimization},
	author       = {Norkin, V. I. and Onishchenko, B. O.},
	year         = 2005,
	month        = {Mar},
	journal      = {Cybernetics and Systems Analysis},
	volume       = 41,
	number       = 2,
	pages        = {203–214},
	doi          = {10.1007/s10559-005-0053-4},
}

@article{Norkin_Pflug_Ruszczynski_1998,
	title        = {A branch and bound method for stochastic global optimization},
	author       = {Norkin, Vladimir I. and Pflug, Georg Ch. and Ruszczyński, Andrzej},
	year         = 1998,
	month        = {Jan},
	journal      = {Mathematical Programming},
	volume       = 83,
	number       = {1–3},
	pages        = {425–450},
	doi          = {10.1007/bf02680569},
}

@inbook{Poliak_1987,
	title        = {The Heavy Ball Method},
	author       = {Poliak, Boris Teodorovich},
	year         = 1987,
	booktitle    = {Introduction to optimization},
	publisher    = {Optimization Software},
	pages        = {65–68},
	place        = {New York},
}

@misc{qian2020,
	title        = {The Impact of the Mini-batch Size on the Variance of Gradients in Stochastic Gradient Descent},
	author       = {Xin Qian and Diego Klabjan},
	year         = 2020,
	url          = {https://arxiv.org/abs/2004.13146},
	eprint       = {2004.13146},
	archiveprefix = {arXiv},
	primaryclass = {math.OC},
}

@article{Radomirovic_2023,
	title        = {Text document clustering approach by improved sine cosine algorithm},
	author       = {Radomirovi\'{c}, Branislav and Jovanovi\'{c}, Vuk and Nikoli\'{c}, Bosko and Stojanovi\'{c}, Sasa and Venkatachalam, K. and Zivkovic, Miodrag and Njegu\v{s}, Angelina and Bacanin, Nebojsa and Strumberger, Ivana},
	year         = 2023,
	month        = jul,
	journal      = {Information Technology and Control},
	volume       = 52,
	number       = 2,
	pages        = {541--561},
	doi          = {10.5755/j01.itc.52.2.33536},
}

@article{Robbins_Monro_1951,
	title        = {A stochastic approximation method},
	author       = {Robbins, Herbert and Monro, Sutton},
	year         = 1951,
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {JSTOR},
	volume       = 22,
	number       = 3,
	pages        = {400–407},
	doi          = {10.1214/aoms/1177729586},
}

@book{Scholkopf_Smola_2002,
	title        = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
	author       = {Sch\"olkopf, Bernhard and Smola, Alexander J.},
	year         = 2002,
	publisher    = {MIT Press},
	place        = {Cambridge, Massachusetts},
}

@article{Sculley_2010,
	title        = {Web-scale K-means clustering},
	author       = {Sculley, D.},
	year         = 2010,
	month        = {Apr},
	journal      = {Proceedings of the 19th international conference on World wide web},
	pages        = {1177–1178},
	doi          = {10.1145/1772690.1772862},
}

@inproceedings{Tang_2017,
	title        = {Convergence rate of stochastic k-means},
	author       = {Tang, Cheng and Monteleoni, Claire},
	year         = 2017,
	month        = {20--22 Apr},
	booktitle    = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	series       = {Proceedings of Machine Learning Research},
	volume       = 54,
	pages        = {1495--1503},
	editor       = {Singh, Aarti and Zhu, Jerry},
}

@article{tieleman2012rmsprop,
	title        = {Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning},
	author       = {Tieleman, Tijmen and Hinton, Geoffrey},
	year         = 2012,
	journal      = {COURSERA Neural Networks Mach. Learn},
	volume       = 17,
}

@inproceedings{vor_der_bruck_pouly_2019,
	title        = {Text Similarity Estimation Based on Word Embeddings and Matrix Norms for Targeted Marketing},
	author       = {vor der Br{\"u}ck, Tim  and Pouly, Marc},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {1827--1836},
	doi          = {10.18653/v1/N19-1181},
	url          = {https://aclanthology.org/N19-1181},
	editor       = {Burstein, Jill  and Doran, Christy  and Solorio, Thamar},
	abstract     = {The prevalent way to estimate the similarity of two documents based on word embeddings is to apply the cosine similarity measure to the two centroids obtained from the embedding vectors associated with the words in each document. Motivated by an industrial application from the domain of youth marketing, where this approach produced only mediocre results, we propose an alternative way of combining the word vectors using matrix norms. The evaluation shows superior results for most of the investigated matrix norms in comparison to both the classical cosine measure and several other document similarity estimates.},
}

@article{walkington_2023,
	title        = {Nesterov's Method for Convex Optimization},
	author       = {Walkington, Noel J.},
	year         = 2023,
	journal      = {SIAM Review},
	volume       = 65,
	number       = 2,
	pages        = {539--562},
	doi          = {10.1137/21M1390037},
}

@article{Wan_2019,
	title        = {Application of K-means algorithm in image compression},
	author       = {Wan, Xing},
	year         = 2019,
	month        = {Jul},
	journal      = {IOP Conference Series: Materials Science and Engineering},
	volume       = 563,
	number       = 5,
	doi          = {10.1088/1757-899x/563/5/052042},
}

@article{Wang_Wei_2020,
	title        = {Research on logistics center location-allocation problem based on two-stage K-means algorithms},
	author       = {Wang, Meng and Wei, Xuejiang},
	year         = 2020,
	month        = {Aug},
	journal      = {Advances in Intelligent Systems and Computing},
	pages        = {52–62},
	doi          = {10.1007/978-3-030-55506-1\_5},
}

@inproceedings{Widodo_2011,
	title        = {Clustering patent document in the field of ICT (Information \& Communication Technology)},
	author       = {Widodo, Agus and Budi, Indra},
	year         = 2011,
	booktitle    = {2011 International Conference on Semantic Technology and Information Retrieval},
	pages        = {203--208},
	doi          = {10.1109/STAIR.2011.5995789},
	keywords     = {Patents;Clustering algorithms;Abstracts;Indexing;Matrix decomposition;Singular value decomposition;Clustering;Patent;Singular Value Decomposition;Kmeans;Information & Communication Technology},
}

@article{Zhao_Lan_Chen_Ngo_2021,
	title        = {K-sums clustering},
	author       = {Zhao, Wan-Lei and Lan, Shi-Ying and Chen, Run-Qing and Ngo, Chong-Wah},
	year         = 2021,
	month        = {Oct},
	journal      = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
	doi          = {10.1145/3459637.3482359},
}
