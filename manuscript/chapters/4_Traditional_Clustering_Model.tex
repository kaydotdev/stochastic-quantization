\section{Traditional Clustering Model}

The optimal clustering problem seeks to find $K$ cluster centers $ \{ y_1, ..., y_K \} $ that minimize the sum of distances from each point $ \xi $ to the nearest center. Lloyd's K-Means algorithm \cite{Lloyd_1982} is a prominent method for solving quantization and clustering problems, with numerous extensions \cite{Jain_2010}. Bottou and Bengio \cite{Bottou_1994} interpreted the K-Means algorithm as an analogue of Newton's method and proposed several stochastic gradient descent algorithms for optimal clustering. These stochastic K-Means algorithms use only a subset of possible $ \xi $ values or even a single element at each iteration.

\begin{definition}
    \label{K-Means} \cite{Lloyd_1982}. K-Means iterative algorithm starts with the set of current cluster centers $ \{ y_k^{\,t}, \> k = 1, ..., K \} $, the feature set $ \{ \xi_i, \> i = 1, ..., I \} $ is subdivided into $ K $ non-intersecting groups $ \{ I_1^{\,t}, ..., I_K^{\,t} \} $: point $ \xi_i $ belongs to the group $ I_k^{\,t} $, if

    \begin{equation}
        \label{kmeans-group:eq}
            \| \xi_i - y_k^{\,t} \|^2 = \min_{1 \leq k^{\,\prime} \leq K} \| \xi_i - y_{k^{\,\prime}}^{\,t} \|^2 .
    \end{equation}

    \noindent Denote $ N_k^{\,t} $ the number of points in group $ I_k^{\,t} $, $\; N_k^{\,t} = 0 $ if $ I_k^{\,t} = \emptyset $, $ \sum_{k=1}^K N_k^{\,t} = I $. Remark that $ I_k^{\,t} $ and $ N_k^{\,t} $ depend on $ y^{\,t} $. K-Means iteratively evaluates next cluster centers $ y^{\,t+1} $ with the estimates:

    \begin{equation}
        \label{kmeans-center-estimation:eq}
            y_{k}^{\,t + 1} = \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} \xi_i, \;\;\;k = 1, ..., K; \;\;\; t = 0, 1, \ldots.
    \end{equation}

    \noindent Remark that we can represent these vectors as

    \begin{eqnarray}
        \label{kmeans-center-alt:eq}
            y_{k}^{\,t + 1} &=& y_k^{\,t} - \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} (y_k^{\,t} - \xi_i) = y_k^{\,t} - \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} y_k^{\,t} + \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} \xi_i \nonumber \\
            &=& \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} \xi_i, \;\;\; k = 1, ..., K; \;\;\; t = 0, 1, \ldots.
    \end{eqnarray}
\end{definition}

In \cite{Bottou_1994} the form of K-Means algorithm (\ref{kmeans-center-alt:eq}) was related to Newton's step for solving at each iteration the smooth quadratic problem

\begin{equation}
    \label{kmeans-newton-form:eq}
        \min_{y_1,\ldots,y_K}\left[F^t(y) = \frac{1}{2} \sum_{k=1}^K \sum_{i \in I_k^t} || \xi_i - y_k ||^2\right].
\end{equation}

\noindent with block diagonal Hessian and with the diagonal number $ 1/N_k^{\,t} $ in block $ k $. Moreover, it is easy to see that (\ref{kmeans-center-alt:eq}) is the exact analytical solution of the unconstrained quadratic minimization problem (\ref{kmeans-newton-form:eq}) under fixed partition $\{I_1^{\,t},\ldots,I_K^{\,t}\}$ of the index set $\{1,\ldots,I\}$.

In that paper it was also considered stochastic batch and online version of the stochastic gradient methods with learning rate $ \frac{1}{t + 1} $ for solving a sequence of problems (\ref{kmeans-newton-form:eq}) but without rigorous convergence analysis.

The initial positions of cluster centers $ \{ y_k^0, \> k = 1, ..., K \} $ are set either at random among $ \{ \xi_i, \> i = 1, ..., I \} $ or according the algorithm K-Means++ \cite{Arthur_Vassilvitskii_2007,Nguyen_Duong_2018}. With K-Means++ initialization strategy, the rate of convergence to local optimum is estimated to be $ \mathbb{E} [F] \leq 8(\ln k + 2 ) F^{*} $, where $ F^{*} $ - an optimal solution \cite{Arthur_Vassilvitskii_2007}. Assume $ \{ y_1^0, ..., y_k^0 \} \> (k<K) $ initial cluster centers have already been chosen. The next center $ y_{k+1}^0 \> (k+1<K) $ is sampled from the set $ \{ \xi_i, \> i = 1, ..., I \} $ with probabilities:

\begin{equation}
    \label{kmeans-plus-plus-init:eq}
        q_j = \frac{\min_{1 \leq s \leq k} || \xi_j - y_s^0 ||^2}{\sum_{i=1}^I \min_{1 \leq s \leq k} || \xi_i - y_s^0 ||^2}
\end{equation}

The next positions of the cluster centers $ \{ y_1^{\,t}, ..., y_K^{\,t} \} $ for $ t > 0 $ are calculated as in the original Lloyd's algorithm (\ref{kmeans-group:eq})-(\ref{kmeans-center-estimation:eq}). Remark that one more (discrete optimization) initialization strategy for choosing $y^{0}$ is proposed in \cite[Sec. 4, Stage 1]{Kuzmenko_Uryasev_2019}.

Sculley \cite{Sculley_2010} addresses the limitation of the Lloyd's algorithm with update rule (\ref{kmeans-center-estimation:eq}), highlighting that objective function $ F(y) $ calculation is expensive for large datasets, due to $ O(K \cdot I \cdot d) $ time complexity for a given feature set $ \{ \xi_i \} $, where $ d $ is the dimensionality of each sample $ \xi_i $. The author proposed a solution by introducing a Mini-Batch K-Means modification to the Lloyd's algorithm. This modification uses a small random subset $\Xi^{t}\subset \Xi$ at iteration $t$ to perform algorithm step (\ref{kmeans-center-estimation:eq}).

Consider problem (\ref{global-sq-objective-fn:eq})-(\ref{global-sq-fn-expansion:eq}) for general $ r\geq 1 $:

\begin{eqnarray}
    \label{lloyd-update-step:eq}
        F(y) = F(y_1, ..., y_K) &=& \sum_{i=1}^I p_i \min_{1 \leq k \leq K} \| y_k - \xi_i \|^r \nonumber \\
        &=& \mathbb{E}_{i \sim p} \min_{1 \leq k \leq K} \| y_k - \xi_i \|^r \rightarrow \min_y.
\end{eqnarray}

Given the objective function (\ref{lloyd-update-step:eq}) is a generalized differentiable function \cite{Norkin_1986}, and to find its optima we utilize a generalized gradient set  calculated by the chain rule:

\begin{equation}
    \label{lloyd-grad-set:eq}
        \partial F(y) = \sum_{i=1}^I p_i \; \partial \min_{1 \leq k \leq K} \| y_k - \xi_i \|^r. 
\end{equation}

Let the set of points $ \{ \xi_i, i = 1, ..., I \} $ is subdivided into non-intersecting subsets $ I_k $, $ k = 1, ..., K $,  such that $ i \in I_k $ if $ \| y_k - \xi_i \| = \min_{k^\prime \in \{ 1, ..., K \}} \| y_{k^\prime} - \xi_i \| $.  Some $ I_k $ may occur empty, for example, in the following case:

\begin{equation}
    \label{lloyd-empty-set-cond:eq}
    \max_{1 \leq i \leq I} \min_{1 \leq k \leq K} \|y_k - \xi_i\| < \min_{1 \leq i \leq I} \|y_k - \xi_i\|
\end{equation}

In this modifications some generalized gradient of function $F(y)$ is $ g(y) = (g_1(y), ..., g_K(y)) $, where:

\begin{equation}
    \label{lloyd-gen-grad-component:eq}
    g_k(y) = \begin{cases}
        \sum_{i \in I_k} r p_i \|y_{k} - \xi_i\|^{r\,-2}(y_{k} - \xi_i), & I_k \neq \emptyset, \\
        0, & I_k = \emptyset. 
    \end{cases}
\end{equation}

The standard (unconstrained) generalized gradient method for solving problem (\ref{lloyd-update-step:eq}) takes on the form (for $ p_i = 1/I $):

\begin{eqnarray}
    \label{lloyd-gen-grad:eq}
    y_k^{\,t+1} &=& y_k^{\,t} - \rho_t \,g_k(y^{\,t}) \nonumber \\
                &=& \begin{cases}
        y_k^{\,t} - \rho_t \frac{r}{I} 
				\sum_{i \in I_k^{\,t}} \|y_k^{\,t} - \xi_i\|^{r\,-2}(y_k^{\,t} - \xi_i), 
				& I_k^{\,t} \neq \emptyset, \\
        y_k^{\,t}, & I_k^{\,t} = \emptyset,
    \end{cases}
\end{eqnarray}

\noindent where $ I_k^{\,t} = \{i: \| y_k^{\,t} - \xi_i \| = \min_{1 \leq  k^{\,\prime} \leq K} \| y_{k^{\,\prime}}^{\,t}-\xi_i \| \} \;\;\; k = 1 , \ldots , K \;\;\; t = 0, 1 , \ldots $. Similarly to the convergence conditions in \ref{sq-convergence-cond:eq}, the generalized gradient method converges under the conditions:

\begin{eqnarray}
    \label{lloyd-gen-grad-convergence:eq}
    \lim_{t\rightarrow\infty}\rho_t=0,\;\;\;\sum_{t=0}^\infty\rho_t=+\infty.
\end{eqnarray}

Let $ N_k^{\,t} $ be the number of elements in $ I_k^{\,t} \neq \emptyset $ at iteration $ t $. If we chose $ \rho_t $ dependent on $ k $, namely, $ \rho_{t\,k} = 0.5 \frac{I}{N_k^{\,t}} $, then for $ r=2$ process (\ref{lloyd-gen-grad:eq}) becomes identical to K-Means one (\ref{kmeans-center-estimation:eq}). However such choice of step multipliers does not guarantee convergence of the method. A more general choice can be $ \rho_{t\,k} = \rho_t \frac{I}{N_k^{\,t}} $ with $ \rho_t \geq 0 $ satisfying conditions (\ref{lloyd-gen-grad-convergence:eq}) and thus $ \lim_{t \to \infty} \max_k \rho_{t\,k} = 0 $.

\subsection{Stochastic K-Means Algorithm}

Let $\tilde{\xi}^t$ be a sampled point from the set $\{\xi_i,\;i=1,2,\ldots,I\}$, $t$ denotes the iteration number. Define some stochastic generalized gradient as $ \tilde{g}(y) = (g_k(y), k = 1, ..., K) $:

\begin{equation}
    \label{stoch-lloyd-grad-component:eq}
    \tilde{g}_k(y) = \begin{cases}
        r\|y_{k} - \tilde{\xi}^t\|^{r\,-2} (y_{k} - \tilde{\xi}^t), 
        & k \in S(\tilde{\xi}^t,y),\\
        0,& k\notin S(\tilde{\xi}^t,y)
    \end{cases}
\end{equation}

\noindent where $ S(\tilde{\xi}^t,y) = \argmin_{1 \leq k \leq K} \| y_k - \tilde{\xi}^t \| $. The stochastic generalized gradient method for solving problem (\ref{lloyd-update-step:eq}) takes on the form:

\begin{equation}
    \label{stoch-lloyd-gen-grad:eq}
    y_k^{\,t+1} = y_k^{\,t} - \rho_t \,\tilde{g}_k(y^{\,t}) = \begin{cases}
        y_k^{\,t} - \rho_t  r\|y_k^{\,t} - \xi_i\|^{r\,-2}(y_k^{\,t} - \xi_i), & k\in S(\tilde{\xi}^t,y^{\,t}), \\
        y_k^{\,t}, & k\notin S(\tilde{\xi}^t,y^{\,t})
    \end{cases}
\end{equation}

\noindent $ k=1,\ldots,K;\;\;\;t=0,1,\ldots, $ where step multipliers $\{\rho_t\}$ satisfy conditions (\ref{sq-convergence-cond:eq}). Recent studies \cite{Tang_2017,Zhao_Lan_Chen_Ngo_2021} have examined stochastic K-Means algorithms as methods for solving corresponding non-convex, non-smooth stochastic optimization problems. However, the convergence properties of these algorithms still require further consideration.

\subsection{Modifications of K-Means algorithm}

An alternative construction of the stochastic K-Means consists in subdividing the iteration process into iteration epochs of length $I$ and using shuffling mechanism at each epoch for choosing random elements $\tilde{\xi}^t$ \cite{bottou2009curiously,montavon2012neural}. 

Robust clustering model assumes solving problem (\ref{global-sq-objective-fn:eq}), (\ref{global-sq-fn-expansion:eq}) with parameter $r\in [1,2)$. Such choice of parameter $r$ makes the quantization and clustering model more robust to outliers. However, then stochastic generalized gradients of the objective function should be calculated by formula (\ref{sq-objective-fn-gradient:eq}) and the stochastic clustering algorithm takes on form (\ref{stoch-lloyd-gen-grad:eq}).

One also can consider a complement to the sequences (\ref{lloyd-gen-grad:eq}) and (\ref{stoch-lloyd-gen-grad:eq}) by the Ces\`aro trajectory averaging \cite{Bottou_Curtis_Nocedal_2018,montavon2012neural}:

\begin{eqnarray}
    \label{kmeans-trajectory-avg:eq}
        \bar{y}_k^{\,t+1} &=& (1 - \sigma_{t+1}) \bar{y}_k^{\,t} + \sigma_{t+1} y_k^{\,t+1}, \nonumber \\
        \sigma_{t+1} &=& \frac{\rho_{t+1}}{\sum_{s=1}^{t+1} \rho_{s}}, \;\;\; k = 1, ..., K, \;\;\;t=0,1,\ldots.
\end{eqnarray}

Conditions of convergence for this averaged sequence were studied in \cite{mikhalevich2024}, in particular, they admit learning rate $ \rho_t $ proportional to $ 1 / \sqrt{t+1} $. A similar approach for K-Means generated sequences (\ref{kmeans-center-estimation:eq}) aims to average sequence by the feature set size $ N_k $:

\begin{eqnarray}
    \label{kmeans-trajectory-avg-alt:eq}
        \tilde{y}_k^{\,t+1} &=& (1 - \tilde{\sigma}_{t+1}) \tilde{y}_k^{\,t} + \tilde{\sigma}_{t+1} y_k^{\,t+1}, \nonumber \\
        \tilde{\sigma}_{k, \,t+1} &=& \frac{1}{N_k^{\,t+1}} \left/\right. \sum_{s=1}^{t+1} \frac{1}{N_k^{\,s}}, \;\;\; k = 1, ..., K.
\end{eqnarray}

The standard K-Means algorithm requires finding the nearest cluster center $ \argmin_{1 \leq k \leq K} \| y_k - \xi_i \| $ to each point $ \{ \xi_i, \> i = 1, ..., I \} $. This can be a time consuming operation in case of very large number $ I $. Moreover, points $ \xi_i $ may be sampled sequentially from some continuous distribution and thus the sample $ \{ \xi_i, \> i = 1, 2, ... \} $ can be potentially arbitrary large. The stochastic algorithm (\ref{stoch-lloyd-gen-grad:eq}) uses only one sampled point $ {\xi}^t $ at each iteration and thus only one problem $ \argmin_{1 \leq k \leq K} \| y_k - \tilde{\xi}^t \| $ is solved at iteration $ t $. But similar to \cite{Sculley_2010}, one can use a Mini-Batch of $ m $ such points $ \{ {\xi}^t_{i_1^t}, ..., {\xi}^t_{i_m^t} \} $ instead of the whole set $\Xi= \{ \xi_i,\;i=1,\ldots,I \} $, $ 1\leq m < I $.
