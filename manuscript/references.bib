@article{Abdi_Williams_2010,
	title        = {Principal component analysis},
	author       = {Abdi, Hervé and Williams, Lynne J.},
	year         = 2010,
	month        = {Jul},
	journal      = {WIREs Computational Statistics},
	volume       = 2,
	number       = 4,
	pages        = {433–459},
	doi          = {10.1002/wics.101},
}

@article{Ansel_2024,
	title        = {Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation},
	author       = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and et al.},
	year         = 2024,
	month        = {Apr},
	journal      = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
	doi          = {10.1145/3620665.3640366},
}

@inproceedings{Arthur_Vassilvitskii_2007,
	title        = {k-means++: the advantages of careful seeding},
	author       = {Arthur, David and Vassilvitskii, Sergei},
	year         = 2007,
	booktitle    = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
	location     = {New Orleans, Louisiana},
	publisher    = {Society for Industrial and Applied Mathematics},
	address      = {USA},
	series       = {SODA '07},
	pages        = {1027–1035},
	isbn         = 9780898716245,
	abstract     = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	numpages     = 9,
}

@article{Babic_2020,
	title        = {A Comparison of Approaches for Measuring the Semantic Similarity of Short Texts Based on Word Embeddings},
	author       = {Babić, Karlo and Guerra, Francesco and Martinčić-Ipšić, Sanda and Meštrović, Ana},
	year         = 2020,
	month        = {Dec.},
	journal      = {Journal of Information and Organizational Sciences},
	volume       = 44,
	number       = 2,
	doi          = {10.31341/jios.44.2.2},
	url          = {//jios.foi.hr/index.php/jios/article/view/1427},
	abstractnote = {&amp;lt;p&amp;gt;Measuring the semantic similarity of texts has a vital role in various tasks from the field of natural language processing. In this paper, we describe a set of experiments we carried out to evaluate and compare the performance of different approaches for measuring the semantic similarity of short texts. We perform a comparison of four models based on word embeddings: two variants of Word2Vec (one based on Word2Vec trained on a specific dataset and the second extending it with embeddings of word senses), FastText, and TF-IDF. Since these models provide word vectors, we experiment with various methods that calculate the semantic similarity of short texts based on word vectors. More precisely, for each of these models, we test five methods for aggregating word embeddings into text embedding. We introduced three methods by making variations of two commonly used similarity measures. One method is an extension of the cosine similarity based on centroids, and the other two methods are variations of the Okapi BM25 function. We evaluate all approaches on the two publicly available datasets: SICK and Lee in terms of the Pearson and Spearman correlation. The results indicate that extended methods perform better from the original in most of the cases.&amp;lt;/p&amp;gt;},
}

@inproceedings{Beohar_2021,
	title        = {Handwritten Digit Recognition of MNIST dataset using Deep Learning state-of-the-art Artificial Neural Network (ANN) and Convolutional Neural Network (CNN)},
	author       = {Beohar, Drishti and Rasool, Akhtar},
	year         = 2021,
	booktitle    = {2021 International Conference on Emerging Smart Computing and Informatics (ESCI)},
	pages        = {542--548},
	doi          = {10.1109/ESCI50559.2021.9396870},
	keywords     = {Deep learning;Training;Handwriting recognition;Feature extraction;Central Processing Unit;Convolutional neural networks;Image classification;Handwritten digit recognition;Convolutional Neural Network (CNN);Deep learning;MNIST dataset;Epochs;Hidden Layers;Stochastic Gradient Descent;Backpropagation},
}

@inproceedings{Bottou_1994,
	title        = {Convergence Properties of the K-Means Algorithms},
	author       = {L'eon Bottou and Yoshua Bengio},
	year         = 1994,
	booktitle    = {Advances in Neural Information Processing Systems 7, {[NIPS} Conference, Denver, Colorado, USA, 1994]},
	publisher    = {{MIT} Press},
	pages        = {585--592},
	editor       = {Gerald Tesauro and David S. Touretzky and Todd K. Leen},
}

@article{Bottou_2010,
	title        = {Large-scale machine learning with stochastic gradient descent},
	author       = {Bottou, Léon},
	year         = 2010,
	journal      = {Proceedings of COMPSTAT’2010},
	pages        = {177–186},
	doi          = {10.1007/978-3-7908-2604-3},
}

@article{Bottou_Curtis_Nocedal_2018,
	title        = {Optimization methods for large-scale machine learning},
	author       = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	year         = 2018,
	journal      = {SIAM Review},
	volume       = 60,
	number       = 2,
	pages        = {223–311},
	doi          = {10.1137/16m1080173},
}

@inproceedings{bottou2009curiously,
	title        = {Curiously fast convergence of some stochastic gradient descent algorithms},
	author       = {Bottou, L{\'e}on},
	year         = 2009,
	booktitle    = {Proceedings of the symposium on learning and data science, Paris},
	volume       = 8,
	pages        = {2624--2633},
	organization = {Citeseer},
}

@inproceedings{Chinchor_1992,
	title        = {MUC-4 evaluation metrics},
	author       = {Chinchor, Nancy},
	year         = 1992,
	booktitle    = {Proceedings of the 4th Conference on Message Understanding},
	location     = {McLean, Virginia},
	publisher    = {Association for Computational Linguistics},
	address      = {USA},
	series       = {MUC4 '92},
	pages        = {22–29},
	doi          = {10.3115/1072064.1072067},
	isbn         = 1558602739,
	abstract     = {The MUC-4 evaluation metrics measure the performance of the message understanding systems. This paper describes the scoring algorithms used to arrive at the metrics as well as the improvements that were made to the MUC-3 methods. MUC-4 evaluation metrics were stricter than those used in MUC-3. Given the differences in scoring between MUC-3 and MUC-4, the MUC-4 systems' scores represent a larger improvement over MUC-3 performance than the numbers themselves suggest.},
	numpages     = 8,
}

@inbook{Deisenroth_Faisal_Ong_2020,
	title        = {Dimensionality Reduction with Principal Component Analysis},
	author       = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
	year         = 2020,
	booktitle    = {Mathematics for Machine Learning},
	publisher    = {Cambridge University Press},
	pages        = {317–347},
	isbn         = 9781108679930,
	place        = {Cambridge, United Kingdom},
}

@article{Duchi_2011,
	title        = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	author       = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year         = 2011,
	month        = {jul},
	journal      = {J. Mach. Learn. Res.},
	publisher    = {JMLR.org},
	volume       = 12,
	number       = {null},
	pages        = {2121–2159},
	doi          = {10.5555/1953048.2021068},
	issn         = {1532-4435},
	issue_date   = {2/1/2011},
	abstract     = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	numpages     = 39,
}

@article{Ermolev_Norkin_1998,
	title        = {Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization},
	author       = {Ermol’ev, Yu. M. and Norkin, V. I.},
	year         = 1998,
	month        = {Mar},
	journal      = {Cybernetics and Systems Analysis},
	volume       = 34,
	number       = 2,
	pages        = {196–215},
	doi          = {10.1007/bf02742069},
}

@article{Ermoliev_Norkin_2003,
	title        = {Solution of nonconvex nonsmooth stochastic optimization problems},
	author       = {Ermoliev, Yu. M. and Norkin, V. I.},
	year         = 2003,
	month        = {Sep},
	journal      = {Cybernetics and Systems Analysis},
	volume       = 39,
	number       = 5,
	pages        = {701–715},
	doi          = {10.1023/b:casa.0000012091.84864.65},
}

@book{ermoliev1976stochastic,
	title        = {Stochastic Programming Methods},
	author       = {Ermoliev, Y.},
	year         = 1976,
	publisher    = {Nauka},
	address      = {Moscow},
}

@article{Gandikota_Kane_Maity_Mazumdar_2022,
	title        = {VqSGD: Vector quantized stochastic gradient descent},
	author       = {Gandikota, Venkata and Kane, Daniel and Maity, Raj Kumar and Mazumdar, Arya},
	year         = 2022,
	month        = {Jul},
	journal      = {IEEE Transactions on Information Theory},
	volume       = 68,
	number       = 7,
	pages        = {4573–4587},
	doi          = {10.1109/tit.2022.3161620},
}

@article{Graf_Luschgy_2000,
	title        = {Foundations of quantization for probability distributions},
	author       = {Graf, Siegfried and Luschgy, Harald},
	year         = 2000,
	journal      = {Lecture Notes in Mathematics},
	series       = 7,
	volume       = 1730,
	number       = 1,
	doi          = {10.1007/bfb0103945},
	collection   = 7,
}

@article{harris2020array,
	title        = {Array programming with {NumPy}},
	author       = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J. van der Walt and Ralf Gommers and Pauli Virtanen and David Cournapeau and Eric Wieser and Julian Taylor and Sebastian Berg and Nathaniel J. Smith and Robert Kern and Matti Picus and Stephan Hoyer and Marten H. van Kerkwijk and Matthew Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and Warren Weckesser and Hameer Abbasi and Christoph Gohlke and Travis E. Oliphant},
	year         = 2020,
	month        = sep,
	journal      = {Nature},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 585,
	number       = 7825,
	pages        = {357--362},
	doi          = {10.1038/s41586-020-2649-2},
}

@inproceedings{Hoffer_2015,
	title        = {Deep Metric Learning Using Triplet Network},
	author       = {Hoffer, Elad and Ailon, Nir},
	year         = 2015,
	booktitle    = {Similarity-Based Pattern Recognition},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {84--92},
	isbn         = {978-3-319-24261-3},
	editor       = {Feragen, Aasa and Pelillo, Marcello and Loog, Marco},
	abstract     = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
}

@article{Hunter_2007,
	title        = {Matplotlib: A 2D graphics environment},
	author       = {Hunter, J. D.},
	year         = 2007,
	journal      = {Computing in Science \& Engineering},
	publisher    = {IEEE COMPUTER SOC},
	volume       = 9,
	number       = 3,
	pages        = {90--95},
	doi          = {10.1109/MCSE.2007.55},
	abstract     = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
}

@article{Jain_2010,
	title        = {Data clustering: 50 years beyond K-means},
	author       = {Jain, Anil K.},
	year         = 2010,
	month        = {Jun},
	journal      = {Pattern Recognition Letters},
	volume       = 31,
	number       = 8,
	pages        = {651–666},
	doi          = {10.1016/j.patrec.2009.09.011},
}

@inproceedings{Khosla_2020,
	title        = {Supervised contrastive learning},
	author       = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	year         = 2020,
	booktitle    = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	location     = {Vancouver, BC, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS '20},
	isbn         = 9781713829546,
	abstract     = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsu-pervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the Ima-geNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations.},
	articleno    = 1567,
	numpages     = 13,
}

@article{kiefer1952stochastic,
	title        = {Stochastic estimation of the maximum of a regression function},
	author       = {Kiefer, Jack and Wolfowitz, Jacob},
	year         = 1952,
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {JSTOR},
	pages        = {462--466},
}

@misc{kingma2017adam,
	title        = {Adam: A Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = 2017,
	url          = {https://doi.org/10.48550/arXiv.1412.6980},
}

@misc{Kozyriev_2024,
	title        = {kaydotdev/stochastic-quantization: Robust clustering on high-dimensional data with stochastic quantization},
	author       = {Kozyriev, Anton},
	year         = 2024,
	month        = {Aug},
	journal      = {GitHub},
	publisher    = {GitHub},
	url          = {https://github.com/kaydotdev/stochastic-quantization},
}

@article{Kriegel_Kröger_Zimek_2009,
	title        = {Clustering high-dimensional data},
	author       = {Kriegel, Hans-Peter and Kröger, Peer and Zimek, Arthur},
	year         = 2009,
	month        = {Mar},
	journal      = {ACM Transactions on Knowledge Discovery from Data},
	volume       = 3,
	number       = 1,
	pages        = {1–58},
	doi          = {10.1145/1497577.1497578},
}

@inproceedings{Krizhevsky_2012,
	title        = {ImageNet classification with deep convolutional neural networks},
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year         = 2012,
	booktitle    = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
	location     = {Lake Tahoe, Nevada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'12},
	pages        = {1097–1105},
	doi          = {10.5555/2999134.2999257},
	abstract     = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	numpages     = 9,
}

@article{Kuzmenko_Uryasev_2019,
	title        = {Kantorovich–Rubinstein Distance Minimization: Application to location problems},
	author       = {Kuzmenko, Viktor and Uryasev, Stan},
	year         = 2019,
	month        = sep,
	journal      = {Springer Optimization and Its Applications},
	volume       = 149,
	pages        = {59–68},
	doi          = {10.1007/978-3-030-22788-3\_3},
}

@article{Lakshmanan_Pichler_2023,
	title        = {Soft quantization using entropic regularization},
	author       = {Lakshmanan, Rajmadan and Pichler, Alois},
	year         = 2023,
	month        = {Oct},
	journal      = {Entropy},
	volume       = 25,
	number       = 10,
	pages        = 1435,
	doi          = {10.3390/e25101435},
}

@article{Lecun_1998,
	title        = {Gradient-based learning applied to document recognition},
	author       = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	year         = 1998,
	journal      = {Proceedings of the IEEE},
	volume       = 86,
	number       = 11,
	pages        = {2278--2324},
	doi          = {10.1109/5.726791},
	keywords     = {Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
}

@article{lecun2010mnist,
	title        = {MNIST handwritten digit database},
	author       = {LeCun, Yann and Cortes, Corinna and Burges, CJ},
	year         = 2010,
	journal      = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
	volume       = 2,
}

@article{Lloyd_1982,
	title        = {Least squares quantization in PCM},
	author       = {Lloyd, S.},
	year         = 1982,
	month        = {Mar},
	journal      = {IEEE Transactions on Information Theory},
	volume       = 28,
	number       = 2,
	pages        = {129–137},
	doi          = {10.1109/tit.1982.1056489},
}

@misc{mikhalevich2024,
	title        = {Methods of Nonconvex Optimization},
	author       = {V. S. Mikhalevich and A. M. Gupal and V. I. Norkin},
	year         = 2024,
	publisher    = {Nauka: Moscow, 1987},
	url          = {https://arxiv.org/abs/2406.10406},
}

@book{montavon2012neural,
	title        = {Neural networks: Tricks of the trade},
	year         = 2012,
	publisher    = {Springer},
	editor       = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve and M{\"u}ller, Klaus-Robert},
}

@article{MURASAKI_ANDO_SHIMAMURA_2022,
	title        = {Semi-supervised representation learning via triplet loss based on explicit class ratio of unlabeled data},
	author       = {Murasaki, Kazuhiko and Ando, Shingo and Shimamura, Jun},
	year         = 2022,
	month        = {Apr},
	journal      = {IEICE Transactions on Information and Systems},
	volume       = {E105.D},
	number       = 4,
	pages        = {778–784},
	doi          = {10.1587/transinf.2021edp7073},
}

@inproceedings{nesterov1983method,
	title        = {A method of solving a convex programming problem with convergence rate {$O(1/k^2)$}},
	author       = {Nesterov, Yurii Evgen'evich},
	year         = 1983,
	booktitle    = {Doklady Akademii Nauk},
	volume       = 269,
	pages        = {543--547},
	organization = {Russian Academy of Sciences},
}

@article{Newton_Yousefian_Pasupathy_2018,
	title        = {Stochastic gradient descent: Recent trends},
	author       = {Newton, David and Yousefian, Farzad and Pasupathy, Raghu},
	year         = 2018,
	month        = {Oct},
	journal      = {Recent Advances in Optimization and Modeling of Contemporary Problems},
	pages        = {193–220},
	doi          = {10.1287/educ.2018.0191},
}

@article{Nguyen_Duong_2018,
	title        = {K-means** - a fast and efficient K-means algorithms},
	author       = {Nguyen, Cuong Duc and Duong, Trong Hai},
	year         = 2018,
	journal      = {International Journal of Intelligent Information and Database Systems},
	volume       = 11,
	number       = 1,
	pages        = 27,
	doi          = {10.1504/ijiids.2018.091595},
}

@article{Norkin_1986,
	title        = {Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization},
	author       = {Norkin, V. I.},
	year         = 1986,
	journal      = {Cybernetics},
	volume       = 22,
	number       = 6,
	pages        = {804–809},
	doi          = {10.1007/bf01068698},
}

@article{Norkin_2021,
	title        = {Stochastic generalized gradient methods for training nonconvex nonsmooth neural networks},
	author       = {Norkin, V. I.},
	year         = 2021,
	journal      = {Cybernetics and Systems Analysis},
	volume       = 57,
	number       = 5,
	pages        = {714--729},
	doi          = {10.984 1007/s10559-021-00397-z},
}

@article{Norkin_Kozyriev_Norkin_2024,
	title        = {Modern stochastic quasi-gradient optimization algorithms},
	author       = {Norkin, Vladimir and Kozyriev, Anton and Norkin, Bogdan},
	year         = 2024,
	month        = {Mar.},
	journal      = {International Scientific Technical Journal "Problems of Control and Informatics"},
	volume       = 69,
	number       = 2,
	pages        = {71–83},
	doi          = {10.34229/1028-0979-2024-2-6},
	url          = {https://jais.net.ua/index.php/files/article/view/228},
	abstractnote = {&amp;lt;p&amp;gt;Stochastic optimization has become a leading method in various fields such as machine learning, neural networks training, and signal processing. These problems are aimed at minimizing the objective function with noisy and uncertain data. Such problems are attributed to stochastic programming. The article comprehensively compares modern quasi-gradient methods of stochastic optimization, illustrates their basic principles, convergence properties, and practical applications. First, basic concepts of gradient descent, stochastic approximation and optimization are introduced, and then optimization methods are explained in detail. Extensions of the basic gradient descent such as Nemirovski’s mirror decent, Polyak’s heavy ball (momentum) and Nesterov’s valley step methods are reviewed. Beside these classical methods, adaptive stochastic gradient methods are analyzed in depth; attention is focused on their ability to dynamically change the learning rate and decent directions depending on the structure of the problem and a course of optimization. The nomenclature of adaptive stochastic gradient methods includes AdaGrad, RMSProp, ADAM. Generalizations of these methods to the case of non-smooth objective function are studied; problems arising in non-smooth optimization landscapes are described. These generalizations exploit the idea of smoothing coming back to Steklov (1907) and consist in approximation of the original objective function by a sequence of close smoothed functions. The latter admit approximation of their gradients in the form of finite differences in random directions. The application of these improved methods in the context of unconditional optimization problems is illustrated and their effectiveness in accelerating convergence and increasing accuracy is demonstrated. In particular, our experiments demonstrate a considerable positive effect of smoothing on the behavior of the methods in case of nonsmooth problems. This benchmarking study aims to provide researchers and practitioners with a deeper understanding of recent advances in stochastic optimization and outline a path for future innovation.&amp;lt;/p&amp;gt;},
}

@article{Norkin_Onishchenko_2005,
	title        = {Minorant methods of stochastic global optimization},
	author       = {Norkin, V. I. and Onishchenko, B. O.},
	year         = 2005,
	month        = {Mar},
	journal      = {Cybernetics and Systems Analysis},
	volume       = 41,
	number       = 2,
	pages        = {203–214},
	doi          = {10.1007/s10559-005-0053-4},
}

@article{Norkin_Pflug_Ruszczynski_1998,
	title        = {A branch and bound method for stochastic global optimization},
	author       = {Norkin, Vladimir I. and Pflug, Georg Ch. and Ruszczyński, Andrzej},
	year         = 1998,
	month        = {Jan},
	journal      = {Mathematical Programming},
	volume       = 83,
	number       = {1–3},
	pages        = {425–450},
	doi          = {10.1007/bf02680569},
}

@article{Pedregosa_2011,
	title        = {Scikit-learn: Machine Learning in {P}ython},
	author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year         = 2011,
	journal      = {Journal of Machine Learning Research},
	volume       = 12,
	pages        = {2825--2830},
}

@inbook{Poliak_1987,
	title        = {The Heavy Ball Method},
	author       = {Poliak, Boris Teodorovich},
	year         = 1987,
	booktitle    = {Introduction to optimization},
	publisher    = {Optimization Software},
	pages        = {65–68},
	place        = {New York},
}

@misc{qi2017,
	title        = {Contrastive-center loss for deep neural networks},
	author       = {Ce Qi and Fei Su},
	year         = 2017,
	url          = {https://arxiv.org/abs/1707.07391},
}

@misc{qian2020,
	title        = {The Impact of the Mini-batch Size on the Variance of Gradients in Stochastic Gradient Descent},
	author       = {Xin Qian and Diego Klabjan},
	year         = 2020,
	url          = {https://arxiv.org/abs/2004.13146},
}

@article{Radomirovic_2023,
	title        = {Text document clustering approach by improved sine cosine algorithm},
	author       = {Radomirovi\'{c}, Branislav and Jovanovi\'{c}, Vuk and Nikoli\'{c}, Bosko and Stojanovi\'{c}, Sasa and Venkatachalam, K. and Zivkovic, Miodrag and Njegu\v{s}, Angelina and Bacanin, Nebojsa and Strumberger, Ivana},
	year         = 2023,
	month        = jul,
	journal      = {Information Technology and Control},
	volume       = 52,
	number       = 2,
	pages        = {541--561},
	doi          = {10.5755/j01.itc.52.2.33536},
}

@article{Robbins_Monro_1951,
	title        = {A stochastic approximation method},
	author       = {Robbins, Herbert and Monro, Sutton},
	year         = 1951,
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {JSTOR},
	volume       = 22,
	number       = 3,
	pages        = {400–407},
	doi          = {10.1214/aoms/1177729586},
}

@book{Scholkopf_Smola_2002,
	title        = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
	author       = {Sch\"olkopf, Bernhard and Smola, Alexander J.},
	year         = 2002,
	publisher    = {MIT Press},
	place        = {Cambridge, Massachusetts},
}

@article{Sculley_2010,
	title        = {Web-scale K-means clustering},
	author       = {Sculley, D.},
	year         = 2010,
	month        = {Apr},
	journal      = {Proceedings of the 19th international conference on World wide web},
	pages        = {1177–1178},
	doi          = {10.1145/1772690.1772862},
}

@inproceedings{Sohn_2016,
	title        = {Improved Deep Metric Learning with Multi-class N-pair Loss Objective},
	author       = {Sohn, Kihyuk},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 29,
	editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
}

@inproceedings{Tang_2017,
	title        = {Convergence rate of stochastic k-means},
	author       = {Tang, Cheng and Monteleoni, Claire},
	year         = 2017,
	month        = {20--22 Apr},
	booktitle    = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	series       = {Proceedings of Machine Learning Research},
	volume       = 54,
	pages        = {1495--1503},
	editor       = {Singh, Aarti and Zhu, Jerry},
}

@article{tieleman2012rmsprop,
	title        = {Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning},
	author       = {Tieleman, Tijmen and Hinton, Geoffrey},
	year         = 2012,
	journal      = {COURSERA Neural Networks Mach. Learn},
	volume       = 17,
}

@article{Turpault_Serizel_Vincent_2019,
	title        = {Semi-supervised triplet loss based learning of ambient audio embeddings},
	author       = {Turpault, Nicolas and Serizel, Romain and Vincent, Emmanuel},
	year         = 2019,
	month        = {May},
	journal      = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	doi          = {10.1109/icassp.2019.8683774},
}

@inproceedings{vor_der_bruck_pouly_2019,
	title        = {Text Similarity Estimation Based on Word Embeddings and Matrix Norms for Targeted Marketing},
	author       = {vor der Br{\"u}ck, Tim  and Pouly, Marc},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {1827--1836},
	doi          = {10.18653/v1/N19-1181},
	url          = {https://aclanthology.org/N19-1181},
	editor       = {Burstein, Jill  and Doran, Christy  and Solorio, Thamar},
	abstract     = {The prevalent way to estimate the similarity of two documents based on word embeddings is to apply the cosine similarity measure to the two centroids obtained from the embedding vectors associated with the words in each document. Motivated by an industrial application from the domain of youth marketing, where this approach produced only mediocre results, we propose an alternative way of combining the word vectors using matrix norms. The evaluation shows superior results for most of the investigated matrix norms in comparison to both the classical cosine measure and several other document similarity estimates.},
}

@article{walkington_2023,
	title        = {Nesterov's Method for Convex Optimization},
	author       = {Walkington, Noel J.},
	year         = 2023,
	journal      = {SIAM Review},
	volume       = 65,
	number       = 2,
	pages        = {539--562},
	doi          = {10.1137/21M1390037},
}

@article{Wan_2019,
	title        = {Application of K-means algorithm in image compression},
	author       = {Wan, Xing},
	year         = 2019,
	month        = {Jul},
	journal      = {IOP Conference Series: Materials Science and Engineering},
	volume       = 563,
	number       = 5,
	doi          = {10.1088/1757-899x/563/5/052042},
}

@article{Wang_Wei_2020,
	title        = {Research on logistics center location-allocation problem based on two-stage K-means algorithms},
	author       = {Wang, Meng and Wei, Xuejiang},
	year         = 2020,
	month        = {Aug},
	journal      = {Advances in Intelligent Systems and Computing},
	pages        = {52–62},
	doi          = {10.1007/978-3-030-55506-1\_5},
}

@inproceedings{Widodo_2011,
	title        = {Clustering patent document in the field of ICT (Information \& Communication Technology)},
	author       = {Widodo, Agus and Budi, Indra},
	year         = 2011,
	booktitle    = {2011 International Conference on Semantic Technology and Information Retrieval},
	pages        = {203--208},
	doi          = {10.1109/STAIR.2011.5995789},
	keywords     = {Patents;Clustering algorithms;Abstracts;Indexing;Matrix decomposition;Singular value decomposition;Clustering;Patent;Singular Value Decomposition;Kmeans;Information & Communication Technology},
}

@misc{xuan2020,
	title        = {Improved Embeddings with Easy Positive Triplet Mining},
	author       = {Hong Xuan and Abby Stylianou and Robert Pless},
	year         = 2020,
	url          = {https://arxiv.org/abs/1904.04370},
}

@article{Zhao_Lan_Chen_Ngo_2021,
	title        = {K-sums clustering},
	author       = {Zhao, Wan-Lei and Lan, Shi-Ying and Chen, Run-Qing and Ngo, Chong-Wah},
	year         = 2021,
	month        = {Oct},
	journal      = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
	doi          = {10.1145/3459637.3482359},
}
