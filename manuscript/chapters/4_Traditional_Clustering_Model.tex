\section{Traditional Clustering Model}

The primary objective of the optimal clustering problem is to identify $K$ cluster centers, denoted as $\{ y_1, \ldots, y_K \}$, that minimize the total distance from each data point $\xi$ to its nearest cluster center. Lloyd's K-Means algorithm \cite{Lloyd_1982} is a well-established approach for addressing quantization and clustering challenges, with various extensions reported in the literature \cite{Jain_2010}. Bottou and Bengio \cite{Bottou_1994} conceptualized the K-Means algorithm as an analogue to Newton's method and introduced several stochastic gradient descent (SGD) algorithms aimed at achieving optimal clustering. These stochastic variants of the K-Means algorithm execute iterations based on either a subset of the dataset or a single data point at a time.

\begin{definition}
    \label{K-Means} 
    \cite{Lloyd_1982}. The iterative K-Means algorithm begins with a set of initial cluster centers $\{ y_k^{\,t}, \> k = 1, \ldots, K \}$. The dataset $\{ \xi_i, \> i = 1, \ldots, I \}$ is partitioned into $K$ mutually exclusive groups $\{ I_1^{\,t}, \ldots, I_K^{\,t} \}$, where a point $\xi_i$ is assigned to group $I_k^{\,t}$ if it satisfies:

    \begin{equation}
        \label{kmeans-group:eq}
        \| \xi_i - y_k^{\,t} \|^2 = \min_{1 \leq k^{\,\prime} \leq K} \| \xi_i - y_{k^{\,\prime}}^{\,t} \|^2.
    \end{equation}

    \noindent Let $N_k^{\,t}$ represent the number of points in group $I_k^{\,t}$, with $N_k^{\,t} = 0$ if $I_k^{\,t} = \emptyset$, and $\sum_{k=1}^K N_k^{\,t} = I$. Notably, both $I_k^{\,t}$ and $N_k^{\,t}$ are contingent upon $y^{\,t}$. The K-Means algorithm iteratively updates the cluster centers $y^{\,t+1}$ using the following estimates:

    \begin{equation}
        \label{kmeans-center-estimation:eq}
        y_{k}^{\,t + 1} = \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} \xi_i, \quad k = 1, \ldots, K; \quad t = 0, 1, \ldots.
    \end{equation}

    \noindent These vectors can alternatively be expressed as:

    \begin{eqnarray}
        \label{kmeans-center-alt:eq}
        y_{k}^{\,t + 1} &=& y_k^{\,t} - \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} (y_k^{\,t} - \xi_i) = y_k^{\,t} - \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} y_k^{\,t} + \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} \xi_i \nonumber \\
        &=& \frac{1}{N_k^{\,t}} \sum_{i \in I_k^{\,t}} \xi_i, \quad k = 1, \ldots, K; \quad t = 0, 1, \ldots.
    \end{eqnarray}
\end{definition}

In \cite{Bottou_1994}, the expression for the K-Means algorithm (\ref{kmeans-center-alt:eq}) was linked to the Newtonian step for solving a smooth quadratic problem at each iteration:

\begin{equation}
    \label{kmeans-newton-form:eq}
    \min_{y_1,\ldots,y_K}\left[F^t(y) = \frac{1}{2} \sum_{k=1}^K \sum_{i \in I_k^t} || \xi_i - y_k ||^2\right].
\end{equation}

\noindent with a block diagonal Hessian with diagonal elements $1/N_k^{\,t}$ in block $k$. Furthermore, it is clear that (\ref{kmeans-center-alt:eq}) provides the exact analytical solution to the unconstrained quadratic minimization problem (\ref{kmeans-newton-form:eq}) under a fixed partition $\{I_1^{\,t},\ldots,I_K^{\,t}\}$ of the index set $\{1,\ldots,I\}$.

The paper additionally explores stochastic batch and online versions of stochastic gradient methods with a learning rate of $\frac{1}{t + 1}$ to solve a sequence of problems (\ref{kmeans-newton-form:eq}), though without a rigorous convergence analysis.

The initial positions of cluster centers $\{ y_k^0, \> k = 1, \ldots, K \}$ are determined either randomly from $\{ \xi_i, \> i = 1, \ldots, I \}$ or via the K-Means++ algorithm \cite{Arthur_Vassilvitskii_2007,Nguyen_Duong_2018}. The convergence rate to a local optimum using K-Means++ is estimated as $\mathbb{E} [F] \leq 8(\ln k + 2 ) F^{*}$, where $F^{*}$ denotes an optimal solution \cite{Arthur_Vassilvitskii_2007}. Assuming $\{ y_1^0, \ldots, y_k^0 \} \> (k<K)$ initial centers are chosen, the subsequent center $y_{k+1}^0 \> (k+1<K)$ is selected from $\{ \xi_i, \> i = 1, \ldots, I \}$ with probabilities:

\begin{equation}
    \label{kmeans-plus-plus-init:eq}
    q_j = \frac{\min_{1 \leq s \leq k} || \xi_j - y_s^0 ||^2}{\sum_{i=1}^I \min_{1 \leq s \leq k} || \xi_i - y_s^0 ||^2}
\end{equation}

Subsequent cluster center positions $\{ y_1^{\,t}, \ldots, y_K^{\,t} \}$ for $t > 0$ are calculated using Lloyd's original algorithm (\ref{kmeans-group:eq})-(\ref{kmeans-center-estimation:eq}). An alternative discrete optimization-based initialization strategy for $y^{0}$ is suggested in \cite[Sec. 4, Stage 1]{Kuzmenko_Uryasev_2019}.

Sculley \cite{Sculley_2010} highlighted the inefficiency of Lloyd's algorithm's update rule (\ref{kmeans-center-estimation:eq}) for large datasets, due to the time complexity $O(K \cdot I \cdot d)$ associated with the feature set $\{ \xi_i \}$, where $d$ represents the dimensionality of each sample $\xi_i$. To address this, the author proposed the Mini-Batch K-Means modification, which leverages a small random subset $\Xi^{t}\subset \Xi$ at iteration $t$ to perform the algorithm step (\ref{kmeans-center-estimation:eq}).

Consider the generalized clustering problem (\ref{global-sq-objective-fn:eq})-(\ref{global-sq-fn-expansion:eq}) for any $r\geq 1$:

\begin{eqnarray}
    \label{lloyd-update-step:eq}
    \begin{aligned}
        F(y) = F(y_1, \ldots, y_K) &= \sum_{i=1}^I p_i \min_{1 \leq k \leq K} \| y_k - \xi_i \|^r \\
        &= \mathbb{E}_{i \sim p} \min_{1 \leq k \leq K} \| y_k - \xi_i \|^r \rightarrow \min_y. 
    \end{aligned}
\end{eqnarray}

With the objective function (\ref{lloyd-update-step:eq}) being a generalized differentiable function \cite{Norkin_1986}, its optima can be found via a generalized gradient set, derived using the chain rule:

\begin{equation}
    \label{lloyd-grad-set:eq}
    \partial F(y) = \sum_{i=1}^I p_i \; \partial \min_{1 \leq k \leq K} \| y_k - \xi_i \|^r. 
\end{equation}

The dataset $\{ \xi_i, i = 1, \ldots, I \}$ is divided into non-overlapping subsets $I_k$, $k = 1, \ldots, K$, such that $i \in I_k$ if $\| y_k - \xi_i \| = \min_{k^\prime \in \{ 1, \ldots, K \}} \| y_{k^\prime} - \xi_i \|$. Some subsets $I_k$ may be empty, as in scenarios where:

\begin{equation}
    \label{lloyd-empty-set-cond:eq}
    \max_{1 \leq i \leq I} \min_{1 \leq k \leq K} \|y_k - \xi_i\| < \min_{1 \leq i \leq I} \|y_k - \xi_i\|
\end{equation}

In these modifications, the generalized gradient of $F(y)$ is $g(y) = (g_1(y), \ldots, g_K(y))$, where:

\begin{equation}
    \label{lloyd-gen-grad-component:eq}
    g_k(y) = \begin{cases}
        \sum_{i \in I_k} r p_i \|y_{k} - \xi_i\|^{r\,-2}(y_{k} - \xi_i), & I_k \neq \emptyset, \\
        0, & I_k = \emptyset. 
    \end{cases}
\end{equation}

The standard (unconstrained) generalized gradient method for solving problem (\ref{lloyd-update-step:eq}) is expressed as (for $p_i = 1/I$):

\begin{eqnarray}
    \label{lloyd-gen-grad:eq}
    \begin{aligned}
        y_k^{\,t+1} &= y_k^{\,t} - \rho_t \,g_k(y^{\,t}) \\
                    &= \begin{cases}
                        y_k^{\,t} - \rho_t \frac{r}{I} \sum_{i \in I_k^{\,t}} \|y_k^{\,t} - \xi_i\|^{r\,-2}(y_k^{\,t} - \xi_i), & I_k^{\,t} \neq \emptyset, \\
                        y_k^{\,t}, & I_k^{\,t} = \emptyset,
                    \end{cases}
    \end{aligned}
\end{eqnarray}

\noindent where $I_k^{\,t} = \{i: \| y_k^{\,t} - \xi_i \| = \min_{1 \leq  k^{\,\prime} \leq K} \| y_{k^{\,\prime}}^{\,t}-\xi_i \| \} \quad k = 1 , \ldots , K \quad t = 0, 1 , \ldots$. Analogous to the convergence conditions in \ref{sq-convergence-cond:eq}, the generalized gradient method is guaranteed to converge under the conditions:

\begin{eqnarray}
    \label{lloyd-gen-grad-convergence:eq}
    \lim_{t\rightarrow\infty}\rho_t=0,\quad\sum_{t=0}^\infty\rho_t=+\infty.
\end{eqnarray}

Let $N_k^{\,t}$ denote the number of elements in $I_k^{\,t} \neq \emptyset$ at iteration $t$. If $\rho_t$ is chosen to be dependent on $k$, specifically, $\rho_{t\,k} = 0.5 \frac{I}{N_k^{\,t}}$, then for $r=2$, the process (\ref{lloyd-gen-grad:eq}) becomes equivalent to the K-Means update rule (\ref{kmeans-center-estimation:eq}). However, this choice does not ensure method convergence. A more generalized choice could be $\rho_{t\,k} = \rho_t \frac{I}{N_k^{\,t}}$ with $\rho_t \geq 0$ satisfying conditions (\ref{lloyd-gen-grad-convergence:eq}), ensuring $\lim_{t \to \infty} \max_k \rho_{t\,k} = 0$.

\subsection{Stochastic K-Means Algorithm}

Let $\tilde{\xi}^t$ denote a point sampled from the set $\{\xi_i,\;i=1,2,\ldots,I\}$, where $t$ represents the iteration number. We define a stochastic generalized gradient $\tilde{g}(y) = (g_k(y), k = 1, ..., K)$ as follows:

\begin{equation}
    \label{stoch-lloyd-grad-component:eq}
    \tilde{g}_k(y) = \begin{cases}
        r\|y_{k} - \tilde{\xi}^t\|^{r-2} (y_{k} - \tilde{\xi}^t), & k \in S(\tilde{\xi}^t,y),\\
        0, & k \notin S(\tilde{\xi}^t,y)
    \end{cases}
\end{equation}

\noindent where $S(\tilde{\xi}^t,y) = \argmin_{1 \leq k \leq K} \| y_k - \tilde{\xi}^t \|$. The stochastic generalized gradient method for solving the problem (\ref{lloyd-update-step:eq}) is formulated as:

\begin{equation}
    \label{stoch-lloyd-gen-grad:eq}
    y_k^{t+1} = y_k^t - \rho_t \tilde{g}_k(y^t) = \begin{cases}
        y_k^t - \rho_t r\|y_k^t - \xi_i\|^{r-2}(y_k^t - \xi_i), & k \in S(\tilde{\xi}^t,y^t), \\
        y_k^t, & k \notin S(\tilde{\xi}^t,y^t)
    \end{cases}
\end{equation}

\noindent for $k=1,\ldots,K$ and $t=0,1,\ldots$, where the step multipliers $\{\rho_t\}$ satisfy the conditions specified (\ref{sq-convergence-cond:eq}). Recent studies by \cite{Tang_2017} and \cite{Zhao_Lan_Chen_Ngo_2021} have investigated stochastic K-Means algorithms as methods for addressing corresponding non-convex, non-smooth stochastic optimization problems. However, the convergence properties of these algorithms remain an area requiring further rigorous analysis and consideration.

\subsection{Modifications of K-Means algorithm}

The stochastic K-Means algorithm can be alternatively constructed by dividing the iteration process into epochs of length $I$ and employing a shuffling mechanism at each epoch to select random elements $\tilde{\xi}^t$ \cite{bottou2009curiously,montavon2012neural}. 

The robust clustering model involves solving the problem described by equations (\ref{global-sq-objective-fn:eq}) and (\ref{global-sq-fn-expansion:eq}) with parameter $r \in [1,2)$. This choice of $r$ enhances the model's resilience to outliers in both quantization and clustering. Consequently, the stochastic generalized gradients of the objective function must be calculated using equation (\ref{sq-objective-fn-gradient:eq}), resulting in the stochastic clustering algorithm taking the form of equation (\ref{stoch-lloyd-gen-grad:eq}).

An additional consideration is the application of Cesaro trajectory averaging \cite{Bottou_Curtis_Nocedal_2018,montavon2012neural} to complement the sequences in equations (\ref{lloyd-gen-grad:eq}) and (\ref{stoch-lloyd-gen-grad:eq}):

\begin{equation}
    \label{kmeans-trajectory-avg:eq}
    \begin{aligned}
        \bar{y}_k^{\,t+1} &= (1 - \sigma_{t+1}) \bar{y}_k^{\,t} + \sigma_{t+1} y_k^{\,t+1}, \\
        \sigma_{t+1} &= \frac{\rho_{t+1}}{\sum_{s=1}^{t+1} \rho_{s}}, \quad k = 1, \ldots, K, \quad t=0,1,\ldots.
    \end{aligned}
\end{equation}

Convergence conditions for this averaged sequence, studied in \cite{mikhalevich2024}, permit a learning rate $\rho_t$ proportional to $1 / \sqrt{t+1}$. A similar approach for K-Means generated sequences (\ref{kmeans-center-estimation:eq}) aims to average the sequence by the feature set size $N_k$:

\begin{equation}
    \label{kmeans-trajectory-avg-alt:eq}
    \begin{aligned}
        \tilde{y}_k^{\,t+1} &= (1 - \tilde{\sigma}_{t+1}) \tilde{y}_k^{\,t} + \tilde{\sigma}_{t+1} y_k^{\,t+1}, \\
        \tilde{\sigma}_{k, \,t+1} &= \frac{1}{N_k^{\,t+1}} \left/ \sum_{s=1}^{t+1} \frac{1}{N_k^{\,s}}, \quad k = 1, \ldots, K. \right.
    \end{aligned}
\end{equation}

The standard K-Means algorithm necessitates finding the nearest cluster center $\argmin_{1 \leq k \leq K} \| y_k - \xi_i \|$ for each point $\{ \xi_i, \> i = 1, \ldots, I \}$. This operation can be computationally expensive for large $I$. Furthermore, if points $\xi_i$ are sequentially sampled from a continuous distribution, the sample $\{ \xi_i, \> i = 1, 2, \ldots \}$ can potentially be arbitrarily large. The stochastic algorithm (\ref{stoch-lloyd-gen-grad:eq}) mitigates this issue by using only one sampled point ${\xi}^t$ per iteration, thus solving only one problem $\argmin_{1 \leq k \leq K} \| y_k - \tilde{\xi}^t \|$ at iteration $t$. However, following \cite{Sculley_2010}, one can employ a Mini-Batch of $m$ points $\{ {\xi}^t_{i_1^t}, \ldots, {\xi}^t_{i_m^t} \}$ instead of the entire set $\Xi= \{ \xi_i,\;i=1,\ldots,I \}$, where $1 \leq m < I$, to balance computational efficiency and algorithmic performance.
