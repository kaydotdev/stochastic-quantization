{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17943251-9d9d-499e-9723-8dc076dfd056",
   "metadata": {},
   "source": [
    "# Training an autoencoder neural network with stochastic quantization model\n",
    "\n",
    "Importing all required third-party dependencies"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "import sq.optim as sq_optim\n",
    "import sq.quantization as sq"
   ],
   "id": "d864cf7556ae38c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Choosing a different algorithm for convolutions computations",
   "id": "1229094b36482c2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "id": "f2cf278b8913ecec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To enforce reproducibility we set the random seed manually",
   "id": "57b3b2b057d7078d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "RANDOM_SEED = 1909614334\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Used random seed: {torch.initial_seed()}\")"
   ],
   "id": "ab3814636d14d433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We set up the computation device for performing optimization",
   "id": "aa4ecc7ad851c84f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\""
   ],
   "id": "d023148bd9d85aa3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will use MNIST dataset of handwritten digits",
   "id": "463d3e1140ff47d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "BATCH_TRAIN = 512\n",
    "BATCH_TEST = 512\n",
    "DATA_DIR = '../../data/'\n",
    "RESULTS_DIR = '../../results/'\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=DATA_DIR, train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=DATA_DIR, train=False, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_TRAIN, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_TEST, shuffle=True)"
   ],
   "id": "6e9feea535875a72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These are examples of handwritten digits from the dataset",
   "id": "2fda4252a6926721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Selecting random samples for the visual showcase\n",
    "row_elements = 5\n",
    "\n",
    "fig, axes = plt.subplots(2, row_elements, figsize=(12, 4))\n",
    "\n",
    "# Display the samples in the image grid\n",
    "for i, (image, label) in enumerate(\n",
    "  itertools.islice(train_dataset, 2 * row_elements)\n",
    "):\n",
    "    row = i // row_elements\n",
    "    col = i % row_elements\n",
    "\n",
    "    axes[row, col].imshow(image.squeeze(), cmap=\"gray\")\n",
    "    axes[row, col].set_title(f\"Element for a class '{label}'\")\n",
    "    axes[row, col].axis(\"off\")\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "614c79a0c8d5ec97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A blueprint for a convolutional autoencoder neural network",
   "id": "32c704739ecfb6ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ConvAutoencoder(nn.Module):    \n",
    "    def __init__(self, latent_dim=16):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(64 * 7 * 7, 64 * 7),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64 * 7, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, latent_dim),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 64 * 7),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64 * 7, 64 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return decoded"
   ],
   "id": "98bb89ac245d6d56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Output dimensions of the decoder network should match the input tensor, to ensure the compatibility of input and output dimensions we will display these tensors on the grid. For untrained network, decoder output is a random noise.",
   "id": "5140052181c40291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LATENT_DIM = 3\n",
    "\n",
    "autoenc_model = ConvAutoencoder(latent_dim=LATENT_DIM).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam([\n",
    "    {'params': autoenc_model.encoder.parameters()},\n",
    "    {'params': autoenc_model.decoder.parameters()}\n",
    "], lr=0.001, weight_decay=1e-05)\n",
    "\n",
    "\n",
    "images, labels = next(iter(test_dataloader))\n",
    "fig, axes = plt.subplots(2, row_elements, figsize=(12, 4))\n",
    "\n",
    "\n",
    "for i, image in enumerate(\n",
    "  itertools.islice(autoenc_model(images.to(device)), 2 * row_elements)\n",
    "):\n",
    "    row = i // row_elements\n",
    "    col = i % row_elements\n",
    "\n",
    "    axes[row, col].imshow(image.cpu().detach().numpy().squeeze(), cmap=\"gray\")\n",
    "    axes[row, col].set_title(f\"Decoded image {i}\")\n",
    "    axes[row, col].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1c399313e6b3cde6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training an instantiated Convolutional Autoencoder",
   "id": "a469d7fc2b0bd9bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_loss, val_loss = [], []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "\n",
    "    # Training loss and accuracy\n",
    "    autoenc_model.train()\n",
    "\n",
    "    for images, _ in (progress_bar := tqdm(train_dataloader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = autoenc_model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss.append(loss.item())\n",
    "        progress_bar.set_description(f\"Train loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss.append(np.mean(np.array(epoch_train_loss)))\n",
    "\n",
    "    # Validation loss and accuracy\n",
    "    autoenc_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_dataloader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = autoenc_model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "\n",
    "            epoch_val_loss.append(loss.item())\n",
    "\n",
    "    val_loss.append(np.mean(np.array(epoch_val_loss)))\n",
    "\n",
    "    print(f\"Validation loss: {loss.item():.4f}\")"
   ],
   "id": "d076572791044f2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We compare train and validation losses to detect the overfitting of the model",
   "id": "d7327c5e1e04915c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n",
    "\n",
    "ax.plot(range(len(train_loss)), train_loss, linestyle='-',\n",
    "           marker='o', color='k', markersize=4, label=\"Train loss\")\n",
    "ax.plot(range(len(val_loss)), val_loss, linestyle='--',\n",
    "           marker='s', color='k', markersize=4, label=\"Validation loss\")\n",
    "ax.legend(loc=\"upper center\")\n",
    "ax.set_title(\"Train and validation loss with each epoch\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "da15916cace981da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The decoder network outputs resemble an input data, which means encoder-decoder pair formed a bijective mapping from a discrete handwritten digit set, into a metric latent space",
   "id": "94600a8c47f1b737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "autoenc_model.eval()\n",
    "\n",
    "row_elements = 5\n",
    "fig, axes = plt.subplots(2, row_elements, figsize=(12, 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(\n",
    "        itertools.islice(train_dataset, row_elements)\n",
    "    ):\n",
    "        axes[0, i].imshow(image.squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].set_title(f\"Original image\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "        inp_image = image.reshape((1, 1, 28, 28)).to(device)\n",
    "\n",
    "        reconst_image = autoenc_model(inp_image).reshape((1, 28, 28)).cpu()\n",
    "\n",
    "        axes[1, i].imshow(reconst_image.squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].set_title(f\"Reconstruct\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "45e5ffda446d0886"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We convert all handwritten digits from the dataset to visualize their embeddings in the metric space",
   "id": "c58fdea49a9897d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def group_embeddings_by_label(model: nn.Module, dataloader: DataLoader):\n",
    "    all_embeddings = np.empty((0, LATENT_DIM))\n",
    "    grouped_embeddings = {i: np.empty((0, LATENT_DIM)) for i in range(10)}\n",
    "\n",
    "    for img, lbs in dataloader:\n",
    "        model_device = next(model.parameters()).device\n",
    "        batch_embeddings = model(img.to(model_device))\n",
    "        batch_embeddings = batch_embeddings.cpu().detach().numpy()\n",
    "\n",
    "        lbs = lbs.detach().numpy()\n",
    "\n",
    "        for emb, lb in zip(batch_embeddings, lbs):\n",
    "            grouped_embeddings[lb] = np.vstack((grouped_embeddings[lb], np.array(emb)))\n",
    "\n",
    "    for cl, embeds in grouped_embeddings.items():\n",
    "        all_embeddings = np.vstack((all_embeddings, np.array(embeds)))\n",
    "\n",
    "    return grouped_embeddings, all_embeddings\n",
    "\n",
    "\n",
    "embedding_model = autoenc_model.encoder\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_group_embeds, train_all_embeds = group_embeddings_by_label(embedding_model, train_dataloader)\n",
    "    test_group_embeds, test_all_embeds = group_embeddings_by_label(embedding_model, test_dataloader)"
   ],
   "id": "2019e30564ae4440"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If our latent space has a dimensionality greater than 3, we use PCA to project embeddings on a 3D plane, while trying to preserve an original distribution as much as possible with 90% of variance. As train data embeddings are positioned similarly to the test embeddings, we can assume that encoder learned low-level prepresentations",
   "id": "9bea18c960c01b32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_, embeds_dim = train_all_embeds.shape\n",
    "\n",
    "if embeds_dim > 3:\n",
    "    pca = PCA(n_components=3, random_state=RANDOM_SEED).fit(train_all_embeds)\n",
    "\n",
    "    train_viz_embeds = {cl: pca.transform(embeds) for cl, embeds in train_group_embeds.items()}\n",
    "    test_viz_embeds = {cl: pca.transform(embeds) for cl, embeds in test_group_embeds.items()}\n",
    "else:\n",
    "    train_viz_embeds = train_group_embeds\n",
    "    test_viz_embeds = test_group_embeds\n",
    "\n",
    "markers = ['o', '^', 's', 'p', '*', 'x', 'D', 'v', 'h', '+']\n",
    "markevery, marksize = 50, 10\n",
    "elev, azim = 60, 90\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.view_init(elev=elev, azim=azim, roll=0)\n",
    "\n",
    "for idx, (cls, embeds) in enumerate(train_viz_embeds.items()):\n",
    "    subset_embeds = embeds[::markevery]\n",
    "    ax.scatter3D(subset_embeds[:, 0], subset_embeds[:, 1], subset_embeds[:, 2], color='k',\n",
    "                 label=cls, alpha=1.0, s=marksize, marker=markers[idx % len(markers)])\n",
    "\n",
    "ax.legend(loc=\"upper center\", ncol=5)\n",
    "ax.set_title(r'Train dataset embeddings with $ \\frac{1}{50} $ density')\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_ylabel('Z-axis')\n",
    "ax.grid(False)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.view_init(elev=elev, azim=azim, roll=0)\n",
    "\n",
    "for idx, (cls, embeds) in enumerate(test_viz_embeds.items()):\n",
    "    subset_embeds = embeds[::markevery]\n",
    "    ax.scatter3D(subset_embeds[:, 0], subset_embeds[:, 1], subset_embeds[:, 2], color='k',\n",
    "                 label=cls, alpha=1.0, s=marksize, marker=markers[idx % len(markers)])\n",
    "\n",
    "ax.legend(loc=\"upper center\", ncol=5)\n",
    "ax.set_title(r'Test dataset embeddings with $ \\frac{1}{50} $ density')\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_ylabel('Z-axis')\n",
    "ax.grid(False)\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "plt.show()"
   ],
   "id": "3d256e31faf44423"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "An autoencoder model is usually applied in the convex of unsupervised learning to create low level representations of unlabeled data, then an algorithm of unsupervised classification performs a grouping of these representations in some metric space. We will use stochastic quantization algorithm for the non-convex optimization problem of representations clustering",
   "id": "cd1b81274f48793a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sq_algorithms = {\n",
    "    \"SGD\": sq.StochasticQuantization(sq_optim.SGDOptimizer(), n_clusters=10, max_iter=5,\n",
    "                                     random_state=np.random.RandomState(RANDOM_SEED)),\n",
    "    \"Momentum\": sq.StochasticQuantization(sq_optim.MomentumOptimizer(), n_clusters=10, max_iter=5,\n",
    "                                          learning_rate=0.0001, random_state=np.random.RandomState(RANDOM_SEED)),\n",
    "    \"NAG\": sq.StochasticQuantization(sq_optim.NAGOptimizer(), n_clusters=10, max_iter=5,\n",
    "                                     learning_rate=0.0001, random_state=np.random.RandomState(RANDOM_SEED)),\n",
    "    \"Adagrad\": sq.StochasticQuantization(sq_optim.AdagradOptimizer(), n_clusters=10, max_iter=5,\n",
    "                                         learning_rate=0.5, random_state=np.random.RandomState(RANDOM_SEED)),\n",
    "    \"RMSProp\": sq.StochasticQuantization(sq_optim.RMSPropOptimizer(), n_clusters=10, max_iter=5,\n",
    "                                         random_state=np.random.RandomState(RANDOM_SEED)),\n",
    "    \"Adam\": sq.StochasticQuantization(sq_optim.AdamOptimizer(), n_clusters=10, max_iter=5,\n",
    "                                      learning_rate=0.1, random_state=np.random.RandomState(RANDOM_SEED))\n",
    "}\n",
    "\n",
    "markers = [('o', '-'), ('s', '--'), ('^', ':'), ('d', '-.'), ('p', (0, (5, 5))), ('h', (0, (1, 1)))]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "for idx, (name, alg) in enumerate(sq_algorithms.items()):\n",
    "    marker, linestyle = markers[idx % len(markers)]\n",
    "\n",
    "    alg = alg.fit(train_all_embeds)\n",
    "\n",
    "    ax.plot(range(len(alg.loss_history_)), alg.loss_history_,\n",
    "            linestyle=linestyle, marker=marker, color='k',\n",
    "            markersize=4, label=name)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.title(\"Convergence rate of each quantization algorithm\")\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Objective function loss')\n",
    "plt.legend(loc=\"upper center\", ncol=2)\n",
    "plt.show()"
   ],
   "id": "ab6963f80a5e0312"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use confusion matrix to compare the accuracy of classification model for each class",
   "id": "8e44bc999989c1a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_col, n_row = (2, 3)\n",
    "fig, ax = plt.subplots(n_col, n_row, figsize=(12, 8))\n",
    "f1_scores = {}\n",
    "\n",
    "for idx, (name, alg) in enumerate(sq_algorithms.items()):\n",
    "    y_true, y_pred = np.array([]), np.array([])\n",
    "\n",
    "    for _, embs in test_group_embeds.items():\n",
    "        cls = alg.predict(embs)\n",
    "        cls_predicted = np.bincount(cls).argmax()\n",
    "\n",
    "        y_true = np.append(y_true, np.full((1, len(embs)), cls_predicted))\n",
    "        y_pred = np.append(y_pred, cls)\n",
    "\n",
    "    ax_idx = ax[idx // n_row, idx % n_row]\n",
    "\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, y_pred, cmap=\"Greys\", colorbar=False, ax=ax_idx\n",
    "    )\n",
    "\n",
    "    ax_idx.set_title(f\"Confusion matrix of a DEC with {name}\")\n",
    "    f1_scores[name] = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a0597d00cf5ac61e"
  },
  {
   "cell_type": "markdown",
   "id": "2284ec40-bc33-409b-9859-030fc5ba5d90",
   "metadata": {},
   "source": "To account for label imbalance in the test set, we choose a weighted F1 score as an accuracy metric."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "bars = ax.bar(f1_scores.keys(), f1_scores.values(), color='k')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2.0,\n",
    "            height, f'{round(height, 2)}', ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel(\"Algorithm\")\n",
    "ax.set_ylabel(\"F1 score\")\n",
    "ax.set_title(\"Comparing accuracy of each stochastic quantization modification\")\n",
    "ax.set_ylim(top=1.0)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "27ecec73959b90e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save encoder model for the later use",
   "id": "d1fedf4c27d3f6d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL_DIR = os.path.join(RESULTS_DIR, \"model\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "embedding_model.eval()\n",
    "\n",
    "torch.save(embedding_model.state_dict(), os.path.join(MODEL_DIR, \"embedding_model.bin\"))"
   ],
   "id": "2ec5d326eb5bdf6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
