\section{Introduction}

Quantization and clustering are fundamental encoding techniques that provide compact representations of original data \cite{Graf_Luschgy_2000,Scholkopf_Smola_2002,Jain_2010}. Clustering algorithms have emerged as prominent tools for unsupervised learning, with recent applications spanning diverse domains such as location-allocation problems \cite{Wang_Wei_2020}, document classification \cite{Widodo_2011,Radomirovic_2023}, and data compression \cite{Wan_2019}.

In this context, we consider a random variable $\xi$ with values in Euclidean space $\mathbb{R}^n$ and distribution $P(d\xi)$, representing the original distribution. The encoded discrete distribution is parameterized by a set of atoms $\{y_1, \ldots, y_K\}$ with corresponding probabilities $\{q_1, \ldots, q_K\}$. The optimal quantization problem aims to find the encoded distribution that minimizes the distance to the original distribution. This mathematical structure is analogous to the optimal clustering problem, where the objective is to determine the positions of $K$ cluster centers $\{y_1, \ldots, y_K\}$ such that the sum of distances from each element $\xi$ to the nearest cluster center is minimized.

The K-means algorithm, proposed by Lloyd \cite{Lloyd_1982}, has been widely used for solving quantization and clustering problems, with numerous extensions \cite{Jain_2010}. Bottou and Bengio \cite{Bottou_1994} interpreted the K-means algorithm as an analogue of Newton's method and proposed several stochastic gradient descent algorithms for solving optimal quantization and clustering problems. However, traditional clustering algorithms are limited by the requirement to load all training data into memory, rendering them non-scalable for large datasets. To address this limitation, Sculley \cite{Sculley_2010} introduced the Mini-batch K-means algorithm, which utilizes only a small subset of possible $\xi$ values at each iteration.

The Stochastic Quantization algorithm reframes the clustering problem as a stochastic transportation problem \cite{Lakshmanan_Pichler_2023,Kuzmenko_Uryasev_2019} by minimizing the distance between elements of the original distribution $\{\xi\}$ and atoms of the encoded discrete distribution $\{y_k\}$. This approach employs Stochastic Gradient Descent (SGD) \cite{ermoliev1976stochastic,kiefer1952stochastic,Robbins_Monro_1951} to search for an optimal minimum, leveraging its computational efficiency in large-scale machine learning problems \cite{Bottou_2010}. The use of stochastic approximation allows the algorithm to update parameters with only one element $\xi$ per iteration, ensuring memory efficiency without compromising convergence to the minimum \cite{Newton_Yousefian_Pasupathy_2018}.

This paper explores advanced modifications of the Stochastic Quantization algorithm, incorporating accelerated variants of SGD \cite{Poliak_1987,nesterov1983method,walkington_2023} and adaptive learning rate techniques \cite{Duchi_Hazan_Singer_2011,tieleman2012rmsprop,kingma2017adam} to enhance convergence speed. Norkin et al. \cite{Norkin_Kozyriev_Norkin_2024} provide a comprehensive comparison of various SGD variants, highlighting their respective advantages and limitations, while also offering convergence speed estimations.

Given that the optimal quantization problem is non-smooth and non-convex, specialized methods are required \cite{Tang_2017,Zhao_Lan_Chen_Ngo_2021,Gandikota_Kane_Maity_Mazumdar_2022}. To validate its convergence, we apply the general theory of non-smooth, non-convex stochastic optimization \cite{mikhalevich2024,Ermolev_Norkin_1998,Ermoliev_Norkin_2003}. While traditional clustering algorithms lack theoretical foundations for convergence guarantees and rely primarily on seeding techniques \cite{Arthur_Vassilvitskii_2007}, stochastic optimization theory provides specific conditions for the local convergence of the Stochastic Quantization algorithm, which we supplement in this research.

To demonstrate the efficiency and scalability of the Stochastic Quantization algorithm, we present an experimental semi-supervised image classification problem using partially labeled data from the MNIST dataset \cite{lecun2010mnist}. We evaluate the trained model accuracy across various ratios of labeled to unlabeled data using the F1-score metric \cite{Chinchor_1992} for multi-label classification. Acknowledging the challenges faced by quantization and clustering algorithms in high-dimensional spaces, such as visualization difficulties and reduced precision with growing number of dimensions \cite{Kriegel_Kr√∂ger_Zimek_2009}, we employ a deep learning model based on the Triplet Network architecture \cite{Hoffer_2015}. This model encodes images into low-dimensional representations in $\mathbb{R}^3$ latent space, providing a basis for comparing the efficiency of both the Stochastic Quantization algorithm and traditional quantization algorithms.
