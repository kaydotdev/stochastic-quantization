\section{Conclusions}

 - We proved the theoretical guarantees of SQ algorithm convergence;
 - By combining contrastive learning with stochastic quantization algorithm, we proposed a scalable clustering approach, which works for high-dimensional data;
 - There is a room for algorithm improvement, e.g., exploring modifications of Stochastic Quantization on finite-difference algorithms, proposed in the paper \cite{Norkin_Kozyriev_Norkin_2024};
 - By utilizing other architectures for data compression, the semi-supervised approach of Stochastic Quantization with Encoding can be transformed into unsupervised learning
 - Mention that there are alternatives to triplet loss objective function \cite{Hoffer_2015} for the contrastive learning, e.g., N-pair loss \cite{Sohn_2016} or center loss \cite{qi2017}, which is the topic for other researchers.