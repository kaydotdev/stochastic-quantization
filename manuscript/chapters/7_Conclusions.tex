\section{Conclusions}

In this paper, we introduced a novel approach to solving semi-supervised learning problems by combining Contrastive Learning with the Stochastic Quantization algorithm. Our robust solution addresses the challenge of scalability in large datasets for clustering problems, while also mitigating the ''curse of dimensionality'' phenomenon through the integration of the Triplet Network. Although we introduced modifications to the Stochastic Quantization algorithm by incorporating an adaptive learning rate, there is potential for further enhancement by developing an alternative modification using the finite-difference algorithm proposed in \cite{Norkin_Kozyriev_Norkin_2024}, which will be the focus of future research. The semi-supervised nature of the problems addressed in this paper arises from the necessity of using labeled data to train the Triplet Network. By employing other Deep Neural Network architectures to produce low-dimensional representations in the latent space, the semi-supervised approach of Stochastic Quantization with Encoding could be extended to unsupervised learning. Additionally, alternative objective functions for contrastive learning, such as N-pair loss \cite{Sohn_2016} or center loss \cite{qi2017}, offer further avenues for exploration in future studies. The results obtained in this paper demonstrate that the proposed approach enables researchers to train classification models on high-dimensional, partially labeled data, significantly reducing the time and labor required for data annotation without substantially compromising accuracy.
