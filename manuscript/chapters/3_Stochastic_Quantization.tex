\section{Stochastic Quantization}

Stochastic Quantization (SQ) represents a paradigm shift from traditional clustering methods by conceptualizing the feature set $\Xi = \{\xi_i, \;i = 1, \ldots, I\}$ and cluster centers $Y = \{y_k, \;k = 1, \ldots, K\}$ as discrete probability distributions. This approach employs the Wasserstein (or Kantorovich-Rubinstein) distance to minimize distortion between these distributions when representing a continuous distribution by a discrete one \cite{Kuzmenko_Uryasev_2019,Lakshmanan_Pichler_2023}. Recent research has extended the application of quantization algorithms to optimal allocation problems for service centers, where atoms of the discrete distribution represent facility and customer locations \cite{Kuzmenko_Uryasev_2019,Norkin_Onishchenko_2005}.

\begin{definition}
    \label{Stochastic Quantization}
    Optimal quantization, as defined by \cite{Kuzmenko_Uryasev_2019}, minimizes the weighted sum of distances between elements of the feature set $\{\xi_i\} \subset \mathbb{R}^{n}$ and centers $\{y_k\} \subset \mathbb{R}^{n}$:

    \begin{equation}
        \label{sq-objective-fn:eq}
        \min_{y = \{ y_1, \ldots, y_K \} \in Y^K \subset \mathbb{R}^{nK}} \min_{q = \{ q_1, \ldots, q_K \} \in \mathbb{R}^K_{+}} \min_{x = \{ x_{ik} \geq 0 \}} \sum_{i=1}^I \sum_{k=1}^K d(\xi_i, y_k)^r x_{ik}
    \end{equation}

    \noindent subject to constraints:

    \begin{equation}
        \label{sq-objective-constraints:eq}
        \sum_{k=1}^K x_{ik} = p_i, \quad \sum_{k=1}^K q_k = 1, \quad i = 1, \ldots, I
    \end{equation}

    \noindent where $p_i > 0, \sum_{i=1}^I p_i = 1$ are normalized supply volumes, $x_{ik}$ are transportation volumes, $d(\xi_i, y_k)_p = \| \xi_i - y_k \|_p = (\sum_{j=1}^n | \xi_{ij} - y_{kj} |^p)^{\frac{1}{p}}$ is the $l_p$ norm defining the distance between elements in the objective function (\ref{sq-objective-fn:eq}), $Y \subset \mathbb{R}^{n}$ is a common constraint set for variables $\{y_k, k = 1, \ldots, K\}$, and $n, I, K \in \mathbb{N}$.
\end{definition}

In this study, we employ the Euclidean norm ($p = 2$) as the distance metric, defined as $d(\xi_i, y_k)_2 = \sqrt{\sum_{j=1}^n | \xi_{ij} - y_{kj} |^2}$. The choice of distance metric may vary depending on the problem domain. For instance, the cosine similarity function $d(\xi_i, y_j)_{\text{cos}} = \cos(\xi_i, y_j) = \frac{\xi_i \cdot y_j}{\| \xi_i \| \cdot \| y_j \|}$ is utilized in text similarity tasks \cite{Babic_2020,vor_der_bruck_pouly_2019}, while Kolmogorov and Levy metrics are employed for probability and risk theory problems \cite{Kuzmenko_Uryasev_2019}.

Given that in the optimal plan, all mass at point $\xi_i$ is transported to the nearest point $y_k$, problem (\ref{sq-objective-fn:eq})-(\ref{sq-objective-constraints:eq}) can be reduced to the following non-convex, non-smooth global stochastic optimization problem:

\begin{equation}
    \label{global-sq-objective-fn:eq}
    \min_{y = \{ y_1, \ldots, y_K \} \in Y^K \subset \mathbb{R}^{nK}} F(y_1, \ldots, y_k)
\end{equation}

\noindent where

\begin{equation}
    \label{global-sq-fn-expansion:eq}
    F(y) = F(y_1, \ldots, y_k) = \sum_{i=1}^I p_i \min_{1 \leq k \leq K} d(\xi_i, y_k)^r = \mathbb{E}_{i \sim p} \min_{1 \leq k \leq K} d(\xi_i, y_k)^r
\end{equation}

Here, $\mathbb{E}_{i \sim p}$ denotes the expected value over the random index $i$ that takes values $\{1, \ldots, I\}$ with probabilities $\{p_1, \ldots, p_I\}$, respectively.

\begin{lemma}
    \label{Lemma 1}
    In the global optimum $y^{*} = (y_1^{*}, \ldots, y_K^{*})$ of (\ref{sq-objective-constraints:eq}), all $\{y_1^{*}, \ldots, y_K^{*}\}$ belong to the convex hull of elements $\{\xi_1, \ldots, \xi_I\}$ in the feature set.
\end{lemma}

\begin{proof}
    The proof proceeds by contradiction. Assume there exists some $y_{k^{*}}^{*} \notin \text{conv}\{\xi_1, \ldots, \xi_I\}$. Let $\bar{y}_{k^{*}}^{*}$ be the projection of $y_{k^{*}}^{*}$ onto $\text{conv}\{\xi_1, \ldots, \xi_I\}$ and consider points $y_{k^{*}}^{*}(\tau) = (1 - \tau)y_{k^{*}}^{*} + \tau\bar{y}_{k^{*}}^{*}$, $\tau \in [0, 1]$. We observe that $\forall\; \xi_i,\; \tau \in (0, 1]: \|y_{k^{*}}^{*}(\tau) - \xi_i\| < \|y_{k^{*}}^{*} - \xi_i\|$. If $\|y_{k^{*}}^{*} - \xi_{i^{*}}\| = \min_{1 \leq k \leq K} \|y_k^{*} - \xi_{i^{*}}\|$ for some $i^{*}$, then

    \begin{equation}
        \min\{\| y_{k^{*}}^{*}(\tau) - \xi_{i^{*}} \|, \min_{k \neq k^{*}} \| y_k^{*} - \xi_{i^{*}} \|\} < \min_k \| y_k^{*} - \xi_{i^{*}} \|
    \end{equation}

    \noindent Thus, $y^{*} = (y_1^{*}, \ldots, y_K^{*})$ is not a local minimum of the objective function (\ref{global-sq-fn-expansion:eq}). Now, consider the case where $\| y_{k^{*}}^{*} - \xi_i \| > \min_k \| y_k^{*} - \xi_i \|$ for all $i$. By assumption, $\min_k \| y_k^{*} - \xi_{i^{\prime}} \|$ for some $i^{\prime}$. The vector $y^{\prime} = (y_1^{*}, \ldots, y_{k^{*} - 1}^{*}, \xi_{i^{\prime}}, y_{k^{*} + 1}^{*}, \ldots, y_K^{*})$ satisfies $F(y^{\prime}) < F(y^{*})$, contradicting the assumption that $y^{*}$ is a minimum. This completes the proof.
\end{proof}

For a continuous probability distribution $P(d\xi)$, we can interpret the objective function (\ref{global-sq-objective-fn:eq}) as a mathematical expectation in a stochastic optimization problem \cite{ermoliev1976stochastic,Newton_Yousefian_Pasupathy_2018,Norkin_Kozyriev_Norkin_2024}:

\begin{equation}
    \label{smooth-stoch-opt-problem:eq}
    \min_{y = \{ y_1, \ldots, y_K \} \in Y^K \subset \mathbb{R}^{nK}} 
    \left[F(y_1, \ldots, y_K) = \mathbb{E} f(y, \xi) = \int_{\xi \in \Xi} f(y, \xi) P(d \xi)\right]
\end{equation}

\noindent with 

\begin{equation}
    \label{smooth-stoch-fn-expansion:eq}
    f(y, \xi) =  \min_{1 \leq k \leq K} d(\xi, y_k)^r, 
\end{equation}

\noindent where the random variable $\xi$ may have a multimodal continuous distribution. The empirical approximation of $F(y)$ in (\ref{smooth-stoch-opt-problem:eq}) is:

\begin{equation}
    \label{empirical-stoch-fn-expansion:eq}
    F_N(y) = \frac{1}{N} \sum_{i=1}^N \min_{1 \leq k \leq K} d(\xi_i, y_k)^r
\end{equation}

\noindent where $\{\tilde{\xi}_j,\; j = 1, \ldots, N\}$ are independent, identically distributed initial samples of the random variable $\xi$. If $K = 1$, $Y$ is convex, and $r\geq 1$, then problem (\ref{global-sq-objective-fn:eq}) is unimodal and reduces to a convex stochastic optimization problem:

\begin{equation}
    \label{convex-stoch-opt-problem:eq}
    \min_{y \in Y} [ F(y) =  \mathbb{E}_{\tilde{i} \sim p} d(\xi_{\tilde{i}}, y)^r ]
\end{equation}

However, for $K \geq 2$, the function $f(\xi, y) = \min_{1 \leq k \leq K} d(\xi, y_k)^r, y = (y_1, \ldots, y_K)$ is the minimum of convex functions and thus is non-smooth and non-convex. In terms of \cite{mikhalevich2024,Norkin_1986}, $f(\xi, y)$ is a random generalized differentiable function of $y$, and its generalized gradient set $\partial_y \,f\,(\xi,y)$ can be calculated by the chain rule:

\begin{eqnarray}
    \label{sq-objective-fn-gradient:eq}
    \begin{aligned}
        \partial_y \,f\,(\xi, y) &= \text{conv.hull} \left\{ \left(\underbrace{\overbrace{0, \ldots, 0, g_{k}}^k(\xi), 0, \ldots, 0}_K\right), \;\; k \in S(\xi, y), \;\; 0 \in \mathbb{R}^n \right\}, \\
        S(\xi, y) &= \{ k: \| \xi - y_{k} \| = \min_{1 \leq k^{\prime} \leq K} \| \xi - y_{k^{\prime}} \| \}, \\
        g_{k}(\xi) &= r \| \xi - y_{k} \|^{r - 2} (y_{k} - \xi)
    \end{aligned}
\end{eqnarray}

The expected value function (\ref{global-sq-fn-expansion:eq}) is also generalized differentiable, and the set-valued mathematical expectation $\mathbb{E}_{\xi} \partial_y\, f\,(\xi, y)$ is a generalized gradient set of function $F$ \cite{mikhalevich2024,Norkin_1986}. Vectors $G(\xi) = (0, \ldots, 0, g_k, 0, \ldots, 0), \;k \in S(\xi, y), \;0 \in \mathbb{R}^n,$ are stochastic generalized gradients of the function $F(y_1, \ldots, y_K)$. These gradients can be utilized to find the optimal solution $y^{*}=(y_1^{*},\ldots,y_K^{*})$ of problems (\ref{global-sq-objective-fn:eq}), (\ref{smooth-stoch-opt-problem:eq}) using Stochastic Gradient Descent (SGD) \cite{Robbins_Monro_1951,kiefer1952stochastic,ermoliev1976stochastic,Norkin_Kozyriev_Norkin_2024} (written in coordinate-wise form):

\begin{eqnarray}
    \label{sgd-update-rule:eq}
    \begin{aligned}
        & y^{t+1}_k = \Pi_{Y} (y^t_k - \rho_t g_k(\tilde{\xi}^t)), \;\;\; \Pi_{Y} (\cdot) = \argmin_{y \in Y} \| \cdot - y\|, \\
        & y^0_k \in Y, \quad k=1,\ldots,K;\quad t=0,1,\ldots, \label{sgd-update-rule2:eq}
    \end{aligned}
\end{eqnarray}

\noindent where $t$ is the iteration number, $\rho_t > 0$ is a learning rate (step) parameter, and $\Pi_{Y}$ is the projection operator onto the set $Y$, $\{\tilde{\xi}^t\}$ are independent identically distributed elements of $\Xi$. The iterative process (\ref{sq-objective-fn-gradient:eq})-(\ref{sgd-update-rule:eq}) for finding the optimal element is summarized in Algorithm \ref{sq:alg}.

\begin{algorithm}
    \caption{Stochastic Quantization}\label{sq:alg}
    \begin{algorithmic}[1]
        \Require $\{ \xi_i, \quad i = 1, \ldots, I \}, \rho, Y, K, T, r$
        \State $y^0 = \{ y_k^0, \quad k = 1, \ldots, K \}$ \Comment{Initialize centers}
        \For{$t \in [0, T - 1]$}
            \State $\tilde{\xi} \in \{ \xi_i, \quad i = 1, \ldots, I \}$ \Comment{Sample an element from the feature set}
            \State $y_k^t, k \in S(\tilde{\xi}, y^t)$ \Comment{Find the nearest center}
            \State $g_k^t = r \| \tilde{\xi} - y_k^t \|^{r - 2} (y_k^t - \tilde{\xi})$ \Comment{Calculate a partial gradient}
            \State $y_k^t := \Pi_Y (y_k^t - \rho g_k^t)$ \Comment{Update the nearest center}
        \EndFor
    \end{algorithmic}
\end{algorithm}

The local convergence conditions of the stochastic generalized gradient method for solving problem (\ref{global-sq-objective-fn:eq}) are described in Theorem \ref{Theorem 1}, with the proof provided in \cite{Ermoliev_Norkin_2003,Ermolev_Norkin_1998}.

\begin{theorem}
    \label{Theorem 1}
    \cite{Ermoliev_Norkin_2003,Ermolev_Norkin_1998}. Consider the iterative sequence $\{ y^t = (y_1^t, \ldots, y_K^t) \}$ formed according to (\ref{sgd-update-rule:eq}), (\ref{sgd-update-rule2:eq}). Let $\{ \tilde{\xi}^t,\;t=0,1,\ldots\}$ be independent sample points from the set $\{ \xi_i, i = 1, \ldots, I \}$ taken with probabilities $\{ p_i, i = 1, \ldots, I \}$; step parameters $\{\rho_t\}$ satisfy conditions:

    \begin{equation}
        \label{sq-convergence-cond:eq}
        \rho_t > 0, \quad \sum_{t=0}^{\infty} \rho_t = \infty, \quad \sum_{t=0}^{\infty} \rho_t^2 < \infty.
    \end{equation}

    Denote $F(Y^{*})$ the set of values of $F$ on critical (stationary) points $Y^{*}$ of problem (\ref{global-sq-objective-fn:eq}), where $Y^{*} = \{ y = (y_1, \ldots, y_K): \partial F(y) \in N_Y (y_1) \times \ldots \times N_Y (y_K) \}$ and $N_Y (y_k)$ represents the normal cone to the set $Y$ at point $y_k$. If $F(Y^{*})$ does not contain intervals and the set $Y$ is a convex compact, then with probability one $\{ y^t \}$ converges to a connected component of $Y^{*}$, and the sequence $\{ F(y^t) \}$ has a limit.
\end{theorem}

While SGD is an efficient local optimization algorithm, the ultimate task is to find global minima of (\ref{global-sq-objective-fn:eq}). A straightforward approach to achieve this is to start the algorithm from different initial points. The research in \cite{Norkin_Pflug_Ruszczynski_1998} proposes a stochastic branch and bound method applicable to the optimization algorithm (\ref{sgd-update-rule:eq}). The idea is to sequentially partition the initial problem into regions (with constraint set $Y_1 \times \ldots \times Y_K$) and use upper and lower bounds to refine partitions with the so-called interchanges relaxation to obtain lower bounds:

\begin{eqnarray}
    \label{sq-branch-bound:eq}
    \begin{aligned}
        \min_{\{ y_k \in Y_k \}} F(y_1, \ldots, y_K)
        &\geq& \sum_{i=1}^I p_i \min_{y \in Y_1\times\ldots\times Y_K} \min_{1 \leq k \leq K} d(\xi_i, y_k)^r \\
        &\geq& \sum_{i=1}^I p_i \min_{1 \leq k \leq K} d(\xi_i, \pi_k(\xi_i))^r,
    \end{aligned}
\end{eqnarray}

\noindent where $\pi_k(\xi_i)=\Pi_{Y_k}(\xi_i)$.

\subsection{Adaptive Stochastic Quantization} \label{adap-stoch-quant:sec}

The minimization of the objective function (\ref{global-sq-objective-fn:eq}) presents a non-smooth, non-convex, and large-scale stochastic optimization problem with multiple local extrema. Although the parameter update process based on Stochastic Gradient Descent (SGD) (\ref{sgd-update-rule:eq}) converges under certain conditions (\ref{sq-convergence-cond:eq}), Qian et al. \cite{qian2020} demonstrated that the variance of gradient oscillations increases proportionally with the size of the training sample:

\begin{equation}
    \label{sgd-oscillations:eq}
    \mathbb{V} (g_{\mathcal{B}_k}) \propto \frac{I^2}{b} \mathbb{V} (g_k),
\end{equation}

\noindent where $\mathbb{V}$ represents the variance over a set, $g_{\mathcal{B}_k} = \frac{1}{b} \sum_{i=1}^{b} g_i (\xi_{\mathcal{B}_i})$ is the averaged gradient over a subset $\xi_{\mathcal{B}_i} \subset \Xi$, and $b = | \xi_{\mathcal{B}_i} |$ is the batch size. These oscillations negatively affect algorithmic stability and reduce convergence speed. Although techniques such as manually adjusting the learning rate $\rho > 0$, employing annealing schedules \cite{Robbins_Monro_1951}, or averaging the gradient over a subset can enhance stability, slow convergence remains a critical limitation of SGD, particularly in high-dimensional models \cite{Norkin_Kozyriev_Norkin_2024}.

Polyak \cite{Poliak_1987} proposed Momentum Gradient Descent (also referred to as the ''Momentum'' or ''Heavy Ball Method'') as a modification to SGD, introducing an acceleration multiplier $0 < \gamma < 1$ to the recurrent sequence (\ref{sgd-update-rule:eq}), drawing on the analogy of physical motion under friction:

\begin{equation}
    \label{momentum-update-rule:eq}
    y^{\,t+1} = y^{\,t} + \gamma (y^{\,t} - y^{\,t-1}) - \rho_t \,g^{\,t}, \quad g^{\,t}\in \partial_y F(y^{\,t}),   \quad t = 1,2,\ldots.
\end{equation}

Nesterov \cite{nesterov1983method,walkington_2023} extended this approach by introducing an extrapolation step for more accurate parameter estimation, known as Nesterov Accelerated Gradient (NAG):

\begin{eqnarray}
    \label{nag-update-rule:eq}
    \begin{aligned}
        \tilde{y}^{\,t} &= y^{\,t} - \rho_t g^{\,t}, \quad g^{\,t} \in \partial_y F(y^{\,t}), \\
        y^{\,t+1} &= \tilde{y}^{\,t} + \gamma (\tilde{y}^{\,t} - \tilde{y}^{\,t-1}), \quad t=1,2,\ldots.
    \end{aligned}
\end{eqnarray}

Both methods (\ref{momentum-update-rule:eq}) and (\ref{nag-update-rule:eq}) have been adapted for non-convex, non-smooth optimization problems \cite{mikhalevich2024}. However, despite their improvement in convergence speed, these modifications often face the vanishing gradient problem when applied to sparse data \cite{Bottou_Curtis_Nocedal_2018}. This issue arises due to the fixed learning rate, which applies equal updates to both significant and insignificant model parameters. To address this, Duchi et al. \cite{Duchi_2011} proposed an adaptive learning rate $\tilde{\rho}_k^{t} = \rho_t / \sqrt{G_k^{\,t} + \varepsilon}$, where the learning rate is normalized over the accumulated gradient to prioritize more significant parameters. This approach, known as AdaGrad, is represented by the update rule:

\begin{equation}
    \label{adagrad-update-rule:eq}
    y^{\,t+1}_k = y^{\,t}_k - \frac{\rho_t}{\sqrt{G_k^{\,t} + \varepsilon}} g^{\,t}_k, \quad k=1,\ldots,K,
\end{equation}

\noindent where $G_k = G_{k-1} + g_{k^*}^2$ is the accumulated sum of squared gradients from previous iterations, and $\varepsilon \ll 10^{-8}$ serves as a smoothing term. Although this method mitigates the convergence issue in sparse datasets, it introduces the challenge of the learning rate decaying too quickly, i.e., $\lim_{k \to \infty} | \tilde{\rho}_k | = 0$. To address this limitation, Tieleman et al. \cite{tieleman2012rmsprop} proposed RMSProp, which normalizes the accumulated gradient using a moving average $G_k = \beta G_{k-1} + (1 - \beta) g_{k^*}^2$. This approach uses a stochastic approximation of the expected value $\mathbb{E} G_k$, controlled by an averaging multiplier $0 < \beta < 1$, to prevent the rapid vanishing of the learning rate.

Further advancements were made by Kingma et al. \cite{kingma2017adam}, who developed the ADAM algorithm, incorporating adaptive moment estimation. The first and second moments of the gradient are estimated as follows:

\begin{eqnarray}
    \label{adam-update-rule:eq}
    \begin{aligned}
        m_k^{\,t} &= \beta_1 m_k^{\,t-1} + (1 - \beta_1) g_k^{\,t}, \\
        v_k^{\,t} &= \beta_2 v_k^{\,t-1} + (1 - \beta_2) \|g_k^{\,t}\|^2, \\
        y_k^{\,t+1} &= y_k^{\,t} - \frac{\rho_t}{\sqrt{v_k^{\,t} + \varepsilon}} m_k^{\,t}, \quad k=1,\ldots,K,
    \end{aligned}
\end{eqnarray}

\noindent where $m_k^{\,t}$ represents the adaptive estimate of the first moment (the expected gradient value), and $v_k^{\,t}$ represents the adaptive estimate of the second moment (the variance). The constants $0 < \beta_1 < 1$ and $0 < \beta_2 < 1$ are used as averaging multipliers. Although $m_k^{\,t}$ and $v_k^{\,t}$ may initially be biased, Kingma et al. introduced bias-corrected estimates:

\begin{equation}
    \label{adam-corrected-estimations:eq}
    \bar{m}_k^{\,t} = \frac{m_k^{\,t}}{1 - \beta_1}, \quad \bar{v}_k^{\,t} = \frac{v_k^{\,t}}{1 - \beta_2}, \quad k=1,\ldots,K.
\end{equation}

Norkin et al. \cite{Norkin_Kozyriev_Norkin_2024} provide a comprehensive review of these adaptive optimization techniques, comparing their convergence properties across a range of problem settings.